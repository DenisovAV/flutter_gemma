{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FunctionGemma → .litertlm Conversion\n\nConverts fine-tuned FunctionGemma model to `.litertlm` format.\n\n**⚠️ CRITICAL Loading Parameters:**\n- `torch_dtype=torch.bfloat16` (NOT float16!)\n- `attn_implementation=\"eager\"`\n\n**Requirements:**\n- A100 GPU runtime\n- Fine-tuned model on Google Drive (folder or ZIP)\n\n**Note:** Uses nightly builds. For stable production, use `.task` format instead."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 1: Install ai-edge-torch-nightly\n# =============================================================================\n!pip uninstall -y tensorflow 2>/dev/null || true\n!pip cache purge\n\n# Install ai-edge-torch packages\n!pip install ai-edge-torch-nightly --force-reinstall --no-cache-dir -q\n!pip install ai-edge-litert-nightly --no-cache-dir -q\n\n# CRITICAL: Install numpy<2.1 AFTER ai-edge-torch (it may override)\n!pip install \"numpy<2.1\" --force-reinstall -q\n\n# Install transformers with pinned version\n!pip install transformers==4.57.3 huggingface_hub sentencepiece -q\n\n# Restore Colab's native Pillow\n!pip install Pillow --force-reinstall -q\n\nprint(\"\\nInstalled:\")\n!pip show ai-edge-torch-nightly | grep Version\n!pip show transformers | grep Version\n!pip show numpy | grep Version\n!pip show Pillow | grep Version\n\nprint(\"\\n⚠️  RESTART RUNTIME after this step! (Runtime → Restart session)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Load fine-tuned model from Google Drive\n",
    "# =============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_NAME = \"functiongemma-flutter-demo-final\"\n",
    "MODEL_DIR = MODEL_NAME\n",
    "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{MODEL_NAME}\"\n",
    "DRIVE_ZIP = f\"/content/drive/MyDrive/{MODEL_NAME}.zip\"\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    print(f\"Found folder: {DRIVE_MODEL_DIR}\")\n",
    "    !cp -r \"{DRIVE_MODEL_DIR}\" .\n",
    "elif os.path.exists(DRIVE_ZIP):\n",
    "    print(f\"Found ZIP: {DRIVE_ZIP}\")\n",
    "    !unzip -q \"{DRIVE_ZIP}\"\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model not found!\\nUpload to: {DRIVE_MODEL_DIR}/ or {DRIVE_ZIP}\")\n",
    "\n",
    "print(f\"\\nModel ready:\")\n",
    "!ls -la \"{MODEL_DIR}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Test Model Before Conversion\n\n**CRITICAL**: Verify the model works BEFORE converting to litertlm.\nIf it outputs garbage here, the problem is in weight loading, not conversion."
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Step 3: Test model BEFORE conversion (using HuggingFace transformers)\n# =============================================================================\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nprint(f\"Loading model from {MODEL_DIR} via HuggingFace transformers...\")\n\n# CRITICAL: Must use same parameters as training!\n# - bfloat16 (NOT float16!)\n# - attn_implementation=\"eager\"\nhf_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_DIR,\n    torch_dtype=torch.bfloat16,           # CRITICAL: same as training!\n    device_map=\"auto\",\n    attn_implementation=\"eager\"            # CRITICAL: same as training!\n)\nhf_model.eval()\nprint(f\"HuggingFace model loaded on {hf_model.device}, dtype={hf_model.dtype}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n# FunctionGemma test prompt\ntest_prompt = \"\"\"<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:change_background_color{description:<escape>Changes background color<escape>,parameters:{properties:{color:{type:<escape>STRING<escape>}},type:<escape>OBJECT<escape>}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTING FINE-TUNED MODEL (HuggingFace)\")\nprint(\"=\" * 50)\nprint(f\"Input: 'make it red'\")\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(hf_model.device)\n\nwith torch.no_grad():\n    outputs = hf_model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(f\"\\nModel output:\")\nprint(response)\nprint(\"=\" * 50)\n\n# Check if output looks valid\nif \"change_background_color\" in response or \"call:\" in response:\n    print(\"✅ Fine-tuned model outputs function call - GOOD!\")\n    print(\"   Proceeding with conversion...\")\nelif \"<pad>\" in response[:50]:\n    print(\"❌ Model outputs <pad> - wrong loading parameters!\")\n    print(\"   Make sure: torch_dtype=bfloat16, attn_implementation='eager'\")\n    raise ValueError(\"STOP: Wrong model loading parameters\")\nelif \"apologize\" in response.lower() or \"sorry\" in response.lower():\n    print(\"❌ Model refuses to call function - fine-tuning didn't work!\")\n    raise ValueError(\"STOP: Model not fine-tuned correctly\")\nelif any(c in response for c in \"为足球收消气\"):\n    print(\"❌ Model outputs garbage - fine-tuning is broken!\")\n    raise ValueError(\"STOP: Model outputs garbage\")\nelse:\n    print(\"⚠️ Unexpected output - review manually\")\n\n# Clean up HF model to free memory before conversion\ndel hf_model\ntorch.cuda.empty_cache()\nprint(\"\\nHuggingFace model unloaded, ready for ai-edge-torch conversion.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Convert to .litertlm\n\nIf the test above shows garbage output, **STOP HERE** - the problem is in `gemma3.build_model_270m()` not loading weights correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 4: Convert to .litertlm format (using official Google parameters)\n# Source: https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n# =============================================================================\nfrom ai_edge_torch.generative.examples.gemma3 import gemma3\nfrom ai_edge_torch.generative.utilities import converter\nfrom ai_edge_torch.generative.utilities.export_config import ExportConfig\nfrom ai_edge_torch.generative.layers import kv_cache\n\n# Load model using ai-edge-torch (required for conversion)\nprint(f\"Loading model from {MODEL_DIR} via ai-edge-torch...\")\npytorch_model = gemma3.build_model_270m(MODEL_DIR)\npytorch_model.eval()\nprint(\"Model loaded!\")\n\nLITERTLM_OUTPUT_DIR = \"litertlm_output\"\nos.makedirs(LITERTLM_OUTPUT_DIR, exist_ok=True)\n\nexport_config = ExportConfig()\nexport_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\nexport_config.mask_as_input = True\n\n# Find tokenizer\nTOKENIZER_PATH = f\"{MODEL_DIR}/tokenizer.model\"\nif not os.path.exists(TOKENIZER_PATH):\n    from huggingface_hub import hf_hub_download\n    TOKENIZER_PATH = hf_hub_download(\n        repo_id=\"google/functiongemma-270m-it\",\n        filename=\"tokenizer.model\"\n    )\nprint(f\"Tokenizer: {TOKENIZER_PATH}\")\n\n# =============================================================================\n# Create FunctionGemma metadata (OFFICIAL Google format)\n# Only 2 stop tokens as per official cookbook\n# =============================================================================\nMETADATA_PATH = f\"{LITERTLM_OUTPUT_DIR}/base_llm_metadata.textproto\"\n\nmetadata_content = r\"\"\"start_token: {\n    token_ids: {\n        ids: [ 2 ]\n    }\n}\nstop_tokens: {\n    token_str: \"<end_of_turn>\"\n}\nstop_tokens: {\n    token_str: \"<start_function_response>\"\n}\nllm_model_type: {\n    function_gemma: {}\n}\n\"\"\"\n\nwith open(METADATA_PATH, 'w') as f:\n    f.write(metadata_content)\nprint(f\"Metadata created: {METADATA_PATH}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Converting to .litertlm...\")\nprint(\"Time: ~5-15 min (A100)\")\nprint(\"=\" * 50)\n\n# Convert with OFFICIAL Google parameters\n# Source: gemma-cookbook/FunctionGemma/Finetune_FunctionGemma_270M_for_Mobile_Actions\ntry:\n    converter.convert_to_litert(\n        pytorch_model,\n        output_path=LITERTLM_OUTPUT_DIR,\n        output_name_prefix=\"functiongemma-flutter\",\n        prefill_seq_len=256,           # Official: 256 (NOT 2048!)\n        kv_cache_max_len=1024,         # Official: 1024 (NOT 4096!)\n        quantize=\"dynamic_int8\",\n        export_config=export_config,\n        output_format=\"litertlm\",\n        tokenizer_model_path=TOKENIZER_PATH,\n        base_llm_metadata_path=METADATA_PATH,  # CRITICAL: base_llm_metadata_path, NOT llm_metadata_path!\n    )\n    print(\"\\n.litertlm conversion complete!\")\nexcept (TypeError, AttributeError) as e:\n    print(f\"\\nlitertlm not supported in this version: {e}\")\n    print(\"Falling back to .tflite...\")\n    converter.convert_to_tflite(\n        pytorch_model,\n        output_path=LITERTLM_OUTPUT_DIR,\n        output_name_prefix=\"functiongemma-flutter\",\n        prefill_seq_len=256,\n        kv_cache_max_len=1024,\n        quantize=\"dynamic_int8\",\n        export_config=export_config,\n    )\n    print(\"\\n.tflite conversion complete\")\n\nprint(\"\\nGenerated files:\")\n!ls -lah {LITERTLM_OUTPUT_DIR}/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Save and Download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 5: Save to Google Drive and download\n# =============================================================================\nimport glob\nimport shutil\nfrom google.colab import files\n\n# Find output files\noutput_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.litertlm\")\nif not output_files:\n    output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.tflite\")\n\nif not output_files:\n    raise FileNotFoundError(\"No output files found!\")\n\nDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\nos.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n\nprint(\"Saving to Google Drive:\")\nfor f in output_files:\n    size = os.path.getsize(f) / 1e6\n    filename = os.path.basename(f)\n    drive_path = f\"{DRIVE_OUTPUT_DIR}/{filename}\"\n    shutil.copy(f, drive_path)\n    print(f\"  {filename} ({size:.1f} MB) -> {drive_path}\")\n\nprint(\"\\nDownloading:\")\nfor f in output_files:\n    files.download(f)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DONE!\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Test Converted Model (Optional)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Step 6: Test the converted model (Optional)\n# =============================================================================\ntry:\n    # For .litertlm we need ai-edge-litert\n    import glob\n    \n    litertlm_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.litertlm\")\n    tflite_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.tflite\")\n    \n    if litertlm_files:\n        print(\"Testing .litertlm with ai-edge-litert...\")\n        # ai-edge-litert test would go here\n        # Currently ai-edge-litert-nightly API is unstable\n        print(\"⚠️ .litertlm testing requires Flutter - test in app instead\")\n        \n    elif tflite_files:\n        print(\"Testing .tflite with MediaPipe...\")\n        !pip install mediapipe -q\n        \n        from mediapipe.tasks.python.genai import llm_inference\n        \n        model_path = tflite_files[0]\n        print(f\"Loading {model_path}...\")\n        \n        options = llm_inference.LlmInferenceOptions(\n            model_path=model_path,\n            max_tokens=128,\n        )\n        engine = llm_inference.LlmInference.create_from_options(options)\n        print(\"Model loaded!\")\n        \n        test_prompts = [\n            \"make the background red\",\n            \"change title to Hello\",\n        ]\n        \n        for prompt in test_prompts:\n            response = engine.generate_response(prompt)\n            print(f\"\\nUser: {prompt}\")\n            print(f\"Model: {response[:100]}...\")\n            \n        print(\"\\n✅ TFLite model works!\")\n    else:\n        print(\"No output files to test\")\n        \nexcept Exception as e:\n    print(f\"Test skipped: {e}\")\n    print(\"Test in Flutter app instead.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}