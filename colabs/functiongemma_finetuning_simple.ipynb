{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FunctionGemma Simple Fine-tuning\n",
        "\n",
        "Fine-tunes FunctionGemma **WITHOUT** system instructions.\n",
        "\n",
        "**Result:** Model that works with just user message → function call.\n",
        "No developer turn, no declarations needed at inference.\n",
        "\n",
        "**Functions:**\n",
        "- `change_background_color` - Changes app background color\n",
        "- `change_app_title` - Changes app title\n",
        "- `show_alert` - Shows alert dialog\n",
        "\n",
        "**Requirements:**\n",
        "- A100 GPU runtime (Runtime → Change runtime type → A100)\n",
        "- HuggingFace account with accepted Gemma license\n",
        "- HuggingFace token with write access"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Install dependencies (use Colab's pre-installed torch)\n",
        "# =============================================================================\n",
        "!pip install -q transformers==4.57.3 datasets accelerate evaluate trl==0.26.2 protobuf sentencepiece\n",
        "!pip install -q huggingface_hub tensorboard\n",
        "\n",
        "print(\"\\nDependencies installed!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. HuggingFace Authentication"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Authenticate with HuggingFace using Colab Secrets\n",
        "# =============================================================================\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Add it via the key icon.\")\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Logged in to HuggingFace!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Upload Training Data\n",
        "\n",
        "**File:** `training_data.jsonl`\n",
        "\n",
        "**Format per line:**\n",
        "```json\n",
        "{\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "if not os.path.exists('training_data.jsonl'):\n",
        "    print(\"Please upload training_data.jsonl:\")\n",
        "    uploaded = files.upload()\n",
        "else:\n",
        "    print(\"training_data.jsonl already exists\")\n",
        "\n",
        "!wc -l training_data.jsonl\n",
        "!head -1 training_data.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare Dataset (SIMPLE - no declarations!)\n",
        "\n",
        "**Key difference from standard approach:**\n",
        "- NO developer turn\n",
        "- NO function declarations\n",
        "- Just: user message → function call"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# =============================================================================\n",
        "# SIMPLE format: user → function call (NO declarations!)\n",
        "# =============================================================================\n",
        "\n",
        "def create_conversation(sample):\n",
        "    \"\"\"\n",
        "    Creates SIMPLE training format without system instructions.\n",
        "    \n",
        "    Input:\n",
        "        {\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", ...}\n",
        "    \n",
        "    Output:\n",
        "        {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": \"make it red\"},\n",
        "                {\"role\": \"assistant\", \"tool_calls\": [...]}\n",
        "            ]\n",
        "            # NO \"tools\" key - model learns functions from examples!\n",
        "        }\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"tool_calls\": [\n",
        "                    {\n",
        "                        \"type\": \"function\",\n",
        "                        \"function\": {\n",
        "                            \"name\": sample[\"tool_name\"],\n",
        "                            \"arguments\": json.loads(sample[\"tool_arguments\"])\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "        ]\n",
        "        # NO tools= here! Model learns from examples directly.\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# Load and convert dataset\n",
        "# =============================================================================\n",
        "raw_data = []\n",
        "with open('training_data.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "print(f\"Loaded {len(raw_data)} raw examples\")\n",
        "\n",
        "dataset = Dataset.from_list(raw_data)\n",
        "dataset = dataset.map(create_conversation, remove_columns=dataset.features)\n",
        "\n",
        "# Split into train/test (90%/10%)\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
        "\n",
        "print(f\"\\nDataset prepared:\")\n",
        "print(f\"   Train: {len(dataset['train'])} examples\")\n",
        "print(f\"   Test:  {len(dataset['test'])} examples\")\n",
        "print(f\"\\nSample (first example):\")\n",
        "print(json.dumps(dataset['train'][0], indent=2))"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Base Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"google/functiongemma-270m-it\"\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "print(f\"\\nModel loaded!\")\n",
        "print(f\"   Parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Device: {model.device}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "OUTPUT_DIR = \"functiongemma-simple\"\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training params\n",
        "    max_length=512,                     # Shorter - no declarations!\n",
        "    packing=False,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    \n",
        "    # Optimizer\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    warmup_ratio=0.1,\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    \n",
        "    # Memory\n",
        "    bf16=True,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Max length: {training_args.max_length} (shorter - no declarations!)\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"   Train examples: {len(dataset['train'])}\")\n",
        "print(f\"   Eval examples: {len(dataset['test'])}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Training complete!\")\n",
        "print(f\"   Final loss: {train_result.training_loss:.4f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Save Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FINAL_MODEL_DIR = f\"{OUTPUT_DIR}-final\"\n",
        "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n",
        "\n",
        "trainer.save_model(FINAL_MODEL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"Model saved locally to {FINAL_MODEL_DIR}/\")\n",
        "\n",
        "!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n",
        "\n",
        "print(f\"\\nModel copied to Google Drive: {DRIVE_MODEL_DIR}/\")\n",
        "!ls -la {DRIVE_MODEL_DIR}/"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test Model (SIMPLE - no declarations!)\n",
        "\n",
        "**Key difference:** Just user message, no developer turn!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Test SIMPLE format - just user message!\n",
        "# =============================================================================\n",
        "\n",
        "test_prompts = [\n",
        "    \"make the background red\",\n",
        "    \"rename the app to Hello World\",\n",
        "    \"show an alert saying welcome\",\n",
        "    \"I want a purple background\",\n",
        "    \"set title to My App\",\n",
        "]\n",
        "\n",
        "print(\"Testing model (SIMPLE format - no declarations!):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    # SIMPLE: just user message, NO developer turn, NO tools!\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        # NO tools= parameter!\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nUser: {prompt}\")\n",
        "    print(f\"Model: {response.strip()}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Validate Reloaded Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "print(\"Unloading current model...\")\n",
        "del model\n",
        "del tokenizer\n",
        "del trainer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\nReloading model from {FINAL_MODEL_DIR}...\")\n",
        "\n",
        "reloaded_model = AutoModelForCausalLM.from_pretrained(\n",
        "    FINAL_MODEL_DIR,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "reloaded_tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n",
        "\n",
        "print(f\"Model reloaded on {reloaded_model.device}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Test RELOADED model (SIMPLE format)\n",
        "# =============================================================================\n",
        "print(\"Testing RELOADED model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "reload_test_prompts = [\n",
        "    \"make the background red\",\n",
        "    \"rename the app to Hello World\",\n",
        "    \"show an alert saying welcome\",\n",
        "]\n",
        "\n",
        "all_passed = True\n",
        "\n",
        "for prompt in reload_test_prompts:\n",
        "    # SIMPLE: just user message!\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    input_text = reloaded_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    inputs = reloaded_tokenizer(input_text, return_tensors=\"pt\").to(reloaded_model.device)\n",
        "\n",
        "    outputs = reloaded_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=reloaded_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    response = reloaded_tokenizer.decode(\n",
        "        outputs[0][inputs['input_ids'].shape[1]:],\n",
        "        skip_special_tokens=False\n",
        "    )\n",
        "\n",
        "    is_valid = \"<start_function_call>\" in response and \"<pad>\" not in response[:50]\n",
        "    status = \"OK\" if is_valid else \"FAIL\"\n",
        "    if not is_valid:\n",
        "        all_passed = False\n",
        "\n",
        "    print(f\"\\n[{status}] User: {prompt}\")\n",
        "    print(f\"   Model: {response.strip()[:100]}...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if all_passed:\n",
        "    print(\"RELOADED MODEL WORKS!\")\n",
        "    print(\"Proceed with TFLite conversion.\")\n",
        "else:\n",
        "    print(\"RELOADED MODEL BROKEN!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Download Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r {FINAL_MODEL_DIR}.zip {FINAL_MODEL_DIR}/\n",
        "\n",
        "from google.colab import files\n",
        "files.download(f\"{FINAL_MODEL_DIR}.zip\")\n",
        "\n",
        "print(f\"\\nDownload started: {FINAL_MODEL_DIR}.zip\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Convert to TFLite using functiongemma_to_tflite.ipynb\")\n",
        "print(\"2. Bundle as .task using functiongemma_tflite_to_task.ipynb\")\n",
        "print(\"3. Update Flutter chat.dart to use SIMPLE format (no declarations)\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
