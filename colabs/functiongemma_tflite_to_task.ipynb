{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TFLite â†’ .task Bundle\n\nBundles `.tflite` model with tokenizer into `.task` format for MediaPipe.\n\n**Pipeline:**\n1. [functiongemma_finetuning.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_finetuning.ipynb) - Fine-tune model âœ…\n2. [functiongemma_to_tflite.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_to_tflite.ipynb) - Convert to TFLite âœ…\n3. **This notebook** - Bundle as .task for Flutter\n\n**Requirements:**\n- `.tflite` file from Step 2\n- `tokenizer.model` file\n- Both saved to Google Drive\n\n**Note:** This notebook uses ONLY mediapipe (no ai-edge-torch) to avoid conflicts."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install MediaPipe\n\nMediaPipe is Google's library for on-device ML inference.\nWe use the `bundler` module to package TFLite model with tokenizer into a single `.task` file.\n\n**Important:** \n- We use mediapipe version 0.10.20\n- numpy<2.1 is required for compatibility\n- **RESTART RUNTIME** after this step!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 1: Install mediapipe (clean environment, no ai-edge-torch)\n# =============================================================================\n!pip install mediapipe==0.10.20 -q\n\n# CRITICAL: Install numpy<2.1 AFTER mediapipe (fixes binary incompatibility)\n!pip install \"numpy<2.1\" --force-reinstall -q\n\nprint(\"Installed:\")\n!pip show mediapipe | grep Version\n!pip show numpy | grep Version\n\nprint(\"\\nâš ï¸  RESTART RUNTIME after this step! (Runtime â†’ Restart session)\")\nprint(\"Then run cells starting from Step 2.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Files from Google Drive\n\nLoad files created in the previous notebook `functiongemma_to_tflite.ipynb`:\n- **TFLite model** â€” converted FunctionGemma 270M\n- **Tokenizer** â€” SentencePiece tokenizer from the base model\n\nIf tokenizer is not found on Drive, we download it from HuggingFace."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 2: Load TFLite and tokenizer from Google Drive\n# =============================================================================\nfrom google.colab import drive\nimport os\n\n# Verify bundler works after restart\nfrom mediapipe.tasks.python.genai import bundler\nprint(\"âœ… bundler module: OK\\n\")\n\ndrive.mount('/content/drive')\n\nDRIVE_INPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\n\n# TFLite model - specific file\nTFLITE_MODEL = f\"{DRIVE_INPUT_DIR}/functiongemma-flutter_q8_ekv1024.tflite\"\n\nif not os.path.exists(TFLITE_MODEL):\n    raise FileNotFoundError(\n        f\"File not found: {TFLITE_MODEL}\\n\"\n        \"Run functiongemma_to_tflite.ipynb first!\"\n    )\n\nprint(f\"TFLite: {TFLITE_MODEL}\")\nprint(f\"Size: {os.path.getsize(TFLITE_MODEL) / 1e6:.1f} MB\")\n\n# Tokenizer\nTOKENIZER_PATH = f\"{DRIVE_INPUT_DIR}/tokenizer.model\"\nif not os.path.exists(TOKENIZER_PATH):\n    print(\"\\nTokenizer not found locally, downloading from HuggingFace...\")\n    from huggingface_hub import hf_hub_download\n    TOKENIZER_PATH = hf_hub_download(\n        repo_id=\"google/functiongemma-270m-it\",\n        filename=\"tokenizer.model\"\n    )\n\nprint(f\"Tokenizer: {TOKENIZER_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Create .task Bundle\n\n`.task` is MediaPipe's format that combines model and tokenizer into a single file.\n\n**Key parameters:**\n- `stop_tokens` â€” tokens that make the model stop generation\n- `<start_function_response>` â€” **critical** stop token for FunctionGemma\n  (model must stop after function call and wait for the result)\n- `prompt_prefix/suffix` â€” wrapping for user input\n\nDocumentation: [FunctionGemma Formatting](https://ai.google.dev/gemma/docs/functiongemma/formatting-and-best-practices)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 3: Create MediaPipe .task bundle\n# =============================================================================\nfrom mediapipe.tasks.python.genai import bundler\n\nTASK_OUTPUT = \"functiongemma-flutter_q8_ekv1024.task\"\n\n# FunctionGemma-specific bundle configuration\n# Per Google docs: <start_function_response> IS a stop token for inference\n# https://ai.google.dev/gemma/docs/functiongemma/formatting-and-best-practices\nconfig = bundler.BundleConfig(\n    tflite_model=TFLITE_MODEL,\n    tokenizer_model=TOKENIZER_PATH,\n    start_token=\"<bos>\",\n    stop_tokens=[\n        \"<end_of_turn>\",\n        \"<start_function_response>\",  # CRITICAL: stop after function call\n    ],\n    output_filename=TASK_OUTPUT,\n    # EMPTY prefix/suffix - FunctionGemma format is handled in Dart code\n    # (needs <start_of_turn>developer FIRST, not user)\n    prompt_prefix=\"\",\n    prompt_suffix=\"\",\n)\n\nprint(\"Bundle configuration:\")\nprint(f\"  Stop tokens: {config.stop_tokens}\")\nprint(f\"  Start token: {config.start_token}\")\nprint(f\"  Prompt prefix: {repr(config.prompt_prefix)}\")\nprint(f\"  Prompt suffix: {repr(config.prompt_suffix)}\")\n\nprint(\"\\nCreating .task bundle...\")\nbundler.create_bundle(config)\n\nprint(f\"\\nâœ… .task file created!\")\n!ls -lah {TASK_OUTPUT}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Save and Download\n\nSave the ready `.task` file:\n1. To Google Drive â€” for future use\n2. Download locally â€” to add to Flutter project\n\nAfter downloading, place the file in `example/assets/models/`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Save to Google Drive and download\n",
    "# =============================================================================\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "task_size = os.path.getsize(TASK_OUTPUT) / 1e6\n",
    "print(f\"Task file: {TASK_OUTPUT} ({task_size:.1f} MB)\")\n",
    "\n",
    "# Save to Drive\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\n",
    "drive_path = f\"{DRIVE_OUTPUT_DIR}/{TASK_OUTPUT}\"\n",
    "shutil.copy(TASK_OUTPUT, drive_path)\n",
    "print(f\"Saved: {drive_path}\")\n",
    "\n",
    "# Download\n",
    "print(f\"\\nDownloading...\")\n",
    "files.download(TASK_OUTPUT)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DONE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Upload to HuggingFace Hub\n\nUpload the `.task` file to HuggingFace for easy sharing and distribution.\n\n**Setup:**\n1. Create a new model repository on [huggingface.co](https://huggingface.co/new)\n2. Add `HF_TOKEN` to Colab Secrets (ðŸ”‘ icon in left panel)\n3. Change `HUB_REPO_ID` to your repository\n4. Uncomment and run the code below",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Optional: Upload to HuggingFace Hub\n# =============================================================================\n# Uncomment the code below to upload\n\n# from huggingface_hub import login, HfApi\n# from google.colab import userdata\n#\n# # Login (uses token from Colab Secrets)\n# HF_TOKEN = userdata.get('HF_TOKEN')\n# login(token=HF_TOKEN)\n#\n# # Upload to HuggingFace\n# HUB_REPO_ID = \"your-username/functiongemma-flutter-task\"  # Change this!\n#\n# api = HfApi()\n# api.create_repo(repo_id=HUB_REPO_ID, exist_ok=True)\n#\n# # Create README with license information (required by Gemma Terms)\n# README_CONTENT = f\"\"\"# FunctionGemma (.task format)\n#\n# Converted version of [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it) for MediaPipe LLM Inference.\n#\n# ## Usage\n#\n# This model is designed for the [flutter_gemma](https://github.com/DenisovAV/flutter_gemma) plugin.\n#\n# ## Modifications\n#\n# - Converted from SafeTensors to TFLite format using ai-edge-torch\n# - Quantized to int8 (dynamic quantization)\n# - Bundled with tokenizer as .task file using MediaPipe bundler\n#\n# ## Files\n#\n# - `{TASK_OUTPUT}` - MediaPipe .task bundle ({task_size:.0f} MB)\n#\n# ## License\n#\n# Gemma is provided under and subject to the Gemma Terms of Use found at https://ai.google.dev/gemma/terms\n#\n# ## Original Model\n#\n# - Source: [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it)\n# - License: [Gemma License](https://ai.google.dev/gemma/terms)\n# - Prohibited Use Policy: [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy)\n# \"\"\"\n#\n# # Upload README\n# api.upload_file(\n#     path_or_fileobj=README_CONTENT.encode(),\n#     path_in_repo=\"README.md\",\n#     repo_id=HUB_REPO_ID,\n# )\n# print(\"âœ… Uploaded: README.md\")\n#\n# # Upload .task file\n# api.upload_file(\n#     path_or_fileobj=TASK_OUTPUT,\n#     path_in_repo=TASK_OUTPUT,\n#     repo_id=HUB_REPO_ID,\n# )\n# print(f\"âœ… Uploaded: {TASK_OUTPUT}\")\n#\n# print(f\"\\nðŸŽ‰ Model uploaded to: https://huggingface.co/{HUB_REPO_ID}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}