{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TFLite ‚Üí .litertlm Conversion\n\nConverts fine-tuned FunctionGemma model to `.litertlm` format for LiteRT-LM runtime.\n\n**Pipeline:**\n1. [functiongemma_finetuning.ipynb](https://colab.research.google.com/github/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_finetuning.ipynb) - Fine-tune model ‚úÖ\n2. **This notebook** - Convert to .litertlm for LiteRT-LM\n\n**Requirements:**\n- A100 GPU runtime\n- Fine-tuned model on Google Drive (folder or ZIP)\n\n**‚ö†Ô∏è CRITICAL Loading Parameters:**\n- `torch_dtype=torch.bfloat16` (NOT float16!)\n- `attn_implementation=\"eager\"`\n\n**Note:** Uses nightly builds. For stable production on iOS/Web, use `.task` format instead."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Dependencies\n\nInstall ai-edge-torch-nightly for model conversion to .litertlm format.\n\n**Important:**\n- We use nightly builds (API may change)\n- numpy<2.1 is required for compatibility\n- **RESTART RUNTIME** after this step!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 1: Install ai-edge-torch-nightly\n# =============================================================================\n!pip uninstall -y tensorflow 2>/dev/null || true\n!pip cache purge\n\n# Install ai-edge-torch packages\n!pip install ai-edge-torch-nightly --force-reinstall --no-cache-dir -q\n!pip install ai-edge-litert-nightly --no-cache-dir -q\n\n# CRITICAL: Install numpy<2.1 AFTER ai-edge-torch (it may override)\n!pip install \"numpy<2.1\" --force-reinstall -q\n\n# Install transformers with pinned version\n!pip install transformers==4.57.3 huggingface_hub sentencepiece -q\n\n# Restore Colab's native Pillow\n!pip install Pillow --force-reinstall -q\n\nprint(\"\\nInstalled:\")\n!pip show ai-edge-torch-nightly | grep Version\n!pip show transformers | grep Version\n!pip show numpy | grep Version\n!pip show Pillow | grep Version\n\nprint(\"\\n‚ö†Ô∏è  RESTART RUNTIME after this step! (Runtime ‚Üí Restart session)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Load Model from Google Drive\n\nLoad fine-tuned model from the previous notebook `functiongemma_finetuning.ipynb`:\n- **Model folder** ‚Äî contains weights, config, tokenizer\n- **Or ZIP archive** ‚Äî compressed model folder\n\nUpload to Google Drive before running this cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Load fine-tuned model from Google Drive\n",
    "# =============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_NAME = \"functiongemma-flutter-demo-final\"\n",
    "MODEL_DIR = MODEL_NAME\n",
    "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{MODEL_NAME}\"\n",
    "DRIVE_ZIP = f\"/content/drive/MyDrive/{MODEL_NAME}.zip\"\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    print(f\"Found folder: {DRIVE_MODEL_DIR}\")\n",
    "    !cp -r \"{DRIVE_MODEL_DIR}\" .\n",
    "elif os.path.exists(DRIVE_ZIP):\n",
    "    print(f\"Found ZIP: {DRIVE_ZIP}\")\n",
    "    !unzip -q \"{DRIVE_ZIP}\"\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model not found!\\nUpload to: {DRIVE_MODEL_DIR}/ or {DRIVE_ZIP}\")\n",
    "\n",
    "print(f\"\\nModel ready:\")\n",
    "!ls -la \"{MODEL_DIR}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Test Model Before Conversion\n\n**CRITICAL**: Verify the model works BEFORE converting to litertlm.\nIf it outputs garbage here, the problem is in weight loading, not conversion."
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Step 3: Test model BEFORE conversion (using HuggingFace transformers)\n# =============================================================================\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nprint(f\"Loading model from {MODEL_DIR} via HuggingFace transformers...\")\n\n# CRITICAL: Must use same parameters as training!\n# - bfloat16 (NOT float16!)\n# - attn_implementation=\"eager\"\nhf_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_DIR,\n    torch_dtype=torch.bfloat16,           # CRITICAL: same as training!\n    device_map=\"auto\",\n    attn_implementation=\"eager\"            # CRITICAL: same as training!\n)\nhf_model.eval()\nprint(f\"HuggingFace model loaded on {hf_model.device}, dtype={hf_model.dtype}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n# FunctionGemma test prompt\ntest_prompt = \"\"\"<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:change_background_color{description:<escape>Changes the app background color<escape>,parameters:{properties:{color:{description:<escape>The color name (red, green, blue, yellow, purple, orange)<escape>,type:<escape>STRING<escape>}},required:[<escape>color<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTING FINE-TUNED MODEL (HuggingFace)\")\nprint(\"=\" * 50)\nprint(f\"Input: 'make it red'\")\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(hf_model.device)\n\nwith torch.no_grad():\n    outputs = hf_model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(f\"\\nModel output:\")\nprint(response)\nprint(\"=\" * 50)\n\n# Check if output looks valid\nif \"change_background_color\" in response or \"call:\" in response:\n    print(\"‚úÖ Fine-tuned model outputs function call - GOOD!\")\n    print(\"   Proceeding with conversion...\")\nelif \"<pad>\" in response[:50]:\n    print(\"‚ùå Model outputs <pad> - wrong loading parameters!\")\n    print(\"   Make sure: torch_dtype=bfloat16, attn_implementation='eager'\")\n    raise ValueError(\"STOP: Wrong model loading parameters\")\nelif \"apologize\" in response.lower() or \"sorry\" in response.lower():\n    print(\"‚ùå Model refuses to call function - fine-tuning didn't work!\")\n    raise ValueError(\"STOP: Model not fine-tuned correctly\")\nelif any(c in response for c in \"‰∏∫Ë∂≥ÁêÉÊî∂Ê∂àÊ∞î\"):\n    print(\"‚ùå Model outputs garbage - fine-tuning is broken!\")\n    raise ValueError(\"STOP: Model outputs garbage\")\nelse:\n    print(\"‚ö†Ô∏è Unexpected output - review manually\")\n\n# Clean up HF model to free memory before conversion\ndel hf_model\ntorch.cuda.empty_cache()\nprint(\"\\nHuggingFace model unloaded, ready for ai-edge-torch conversion.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Convert to .litertlm\n\nIf the test above shows garbage output, **STOP HERE** - the problem is in `gemma3.build_model_270m()` not loading weights correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 4: Convert to .litertlm format (using official Google parameters)\n# Source: https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n# =============================================================================\nfrom ai_edge_torch.generative.examples.gemma3 import gemma3\nfrom ai_edge_torch.generative.utilities import converter\nfrom ai_edge_torch.generative.utilities.export_config import ExportConfig\nfrom ai_edge_torch.generative.layers import kv_cache\n\n# Load model using ai-edge-torch (required for conversion)\nprint(f\"Loading model from {MODEL_DIR} via ai-edge-torch...\")\npytorch_model = gemma3.build_model_270m(MODEL_DIR)\npytorch_model.eval()\nprint(\"Model loaded!\")\n\nLITERTLM_OUTPUT_DIR = \"litertlm_output\"\nos.makedirs(LITERTLM_OUTPUT_DIR, exist_ok=True)\n\nexport_config = ExportConfig()\nexport_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\nexport_config.mask_as_input = True\n\n# Find tokenizer\nTOKENIZER_PATH = f\"{MODEL_DIR}/tokenizer.model\"\nif not os.path.exists(TOKENIZER_PATH):\n    from huggingface_hub import hf_hub_download\n    TOKENIZER_PATH = hf_hub_download(\n        repo_id=\"google/functiongemma-270m-it\",\n        filename=\"tokenizer.model\"\n    )\nprint(f\"Tokenizer: {TOKENIZER_PATH}\")\n\n# =============================================================================\n# Create FunctionGemma metadata (OFFICIAL Google format)\n# Only 2 stop tokens as per official cookbook\n# =============================================================================\nMETADATA_PATH = f\"{LITERTLM_OUTPUT_DIR}/base_llm_metadata.textproto\"\n\nmetadata_content = r\"\"\"start_token: {\n    token_ids: {\n        ids: [ 2 ]\n    }\n}\nstop_tokens: {\n    token_str: \"<end_of_turn>\"\n}\nstop_tokens: {\n    token_str: \"<start_function_response>\"\n}\nllm_model_type: {\n    function_gemma: {}\n}\n\"\"\"\n\nwith open(METADATA_PATH, 'w') as f:\n    f.write(metadata_content)\nprint(f\"Metadata created: {METADATA_PATH}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Converting to .litertlm...\")\nprint(\"Time: ~5-15 min (A100)\")\nprint(\"=\" * 50)\n\n# Convert with OFFICIAL Google parameters\n# Source: gemma-cookbook/FunctionGemma/Finetune_FunctionGemma_270M_for_Mobile_Actions\ntry:\n    converter.convert_to_litert(\n        pytorch_model,\n        output_path=LITERTLM_OUTPUT_DIR,\n        output_name_prefix=\"functiongemma-flutter\",\n        prefill_seq_len=256,           # Official: 256 (NOT 2048!)\n        kv_cache_max_len=1024,         # Official: 1024 (NOT 4096!)\n        quantize=\"dynamic_int8\",\n        export_config=export_config,\n        output_format=\"litertlm\",\n        tokenizer_model_path=TOKENIZER_PATH,\n        base_llm_metadata_path=METADATA_PATH,  # CRITICAL: base_llm_metadata_path, NOT llm_metadata_path!\n    )\n    print(\"\\n.litertlm conversion complete!\")\nexcept (TypeError, AttributeError) as e:\n    print(f\"\\nlitertlm not supported in this version: {e}\")\n    print(\"Falling back to .tflite...\")\n    converter.convert_to_tflite(\n        pytorch_model,\n        output_path=LITERTLM_OUTPUT_DIR,\n        output_name_prefix=\"functiongemma-flutter\",\n        prefill_seq_len=256,\n        kv_cache_max_len=1024,\n        quantize=\"dynamic_int8\",\n        export_config=export_config,\n    )\n    print(\"\\n.tflite conversion complete\")\n\nprint(\"\\nGenerated files:\")\n!ls -lah {LITERTLM_OUTPUT_DIR}/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Save and Download\n\nSave the ready `.litertlm` file:\n1. To Google Drive ‚Äî for future use\n2. Download locally ‚Äî to use with LiteRT-LM runtime\n\nAfter downloading, you can use the model with:\n- [CLI tool `lit`](https://github.com/google-ai-edge/LiteRT-LM/releases)\n- Kotlin API for Android/JVM\n- C++ API for native integration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 5: Save to Google Drive and download\n# =============================================================================\nimport glob\nimport shutil\nfrom google.colab import files\n\n# Find output files\noutput_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.litertlm\")\nif not output_files:\n    output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.tflite\")\n\nif not output_files:\n    raise FileNotFoundError(\"No output files found!\")\n\nDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\nos.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n\nprint(\"Saving to Google Drive:\")\nfor f in output_files:\n    size = os.path.getsize(f) / 1e6\n    filename = os.path.basename(f)\n    drive_path = f\"{DRIVE_OUTPUT_DIR}/{filename}\"\n    shutil.copy(f, drive_path)\n    print(f\"  {filename} ({size:.1f} MB) -> {drive_path}\")\n\nprint(\"\\nDownloading:\")\nfor f in output_files:\n    files.download(f)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DONE!\")\nprint(\"=\" * 50)"
  },
  {
   "cell_type": "markdown",
   "source": "## Optional: Upload to HuggingFace Hub\n\nUpload the `.litertlm` file to HuggingFace for easy sharing and distribution.\n\n**Setup:**\n1. Create a new model repository on [huggingface.co](https://huggingface.co/new)\n2. Add `HF_TOKEN` to Colab Secrets (üîë icon in left panel)\n3. Change `HUB_REPO_ID` to your repository\n4. Uncomment and run the code below",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Optional: Upload to HuggingFace Hub\n# =============================================================================\n# Uncomment the code below to upload\n\n# from huggingface_hub import login, HfApi\n# from google.colab import userdata\n# import glob\n# import os\n#\n# # Login (uses token from Colab Secrets)\n# HF_TOKEN = userdata.get('HF_TOKEN')\n# login(token=HF_TOKEN)\n#\n# # Upload to HuggingFace\n# HUB_REPO_ID = \"your-username/functiongemma-flutter-litertlm\"  # Change this!\n#\n# api = HfApi()\n# api.create_repo(repo_id=HUB_REPO_ID, exist_ok=True)\n#\n# # Find output file\n# output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.litertlm\")\n# if not output_files:\n#     output_files = glob.glob(f\"{LITERTLM_OUTPUT_DIR}/*.tflite\")\n# OUTPUT_FILE = output_files[0]\n# output_filename = os.path.basename(OUTPUT_FILE)\n# output_size = os.path.getsize(OUTPUT_FILE) / 1e6\n#\n# # Create README with license information (required by Gemma Terms)\n# README_CONTENT = f\"\"\"# FunctionGemma (.litertlm format)\n#\n# Converted version of [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it) for LiteRT-LM runtime.\n#\n# ## Usage\n#\n# This model is designed for the [flutter_gemma](https://github.com/DenisovAV/flutter_gemma) plugin and LiteRT-LM runtime.\n#\n# ## Modifications\n#\n# - Converted from SafeTensors to .litertlm format using ai-edge-torch-nightly\n# - Quantized to int8 (dynamic quantization)\n# - Bundled with tokenizer and FunctionGemma metadata\n#\n# ## Files\n#\n# - `{output_filename}` - LiteRT-LM bundle ({output_size:.0f} MB)\n#\n# ## License\n#\n# Gemma is provided under and subject to the Gemma Terms of Use found at https://ai.google.dev/gemma/terms\n#\n# ## Original Model\n#\n# - Source: [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it)\n# - License: [Gemma License](https://ai.google.dev/gemma/terms)\n# - Prohibited Use Policy: [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy)\n# \"\"\"\n#\n# # Upload README\n# api.upload_file(\n#     path_or_fileobj=README_CONTENT.encode(),\n#     path_in_repo=\"README.md\",\n#     repo_id=HUB_REPO_ID,\n# )\n# print(\"‚úÖ Uploaded: README.md\")\n#\n# # Upload .litertlm file\n# api.upload_file(\n#     path_or_fileobj=OUTPUT_FILE,\n#     path_in_repo=output_filename,\n#     repo_id=HUB_REPO_ID,\n# )\n# print(f\"‚úÖ Uploaded: {output_filename}\")\n#\n# print(f\"\\nüéâ Model uploaded to: https://huggingface.co/{HUB_REPO_ID}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}