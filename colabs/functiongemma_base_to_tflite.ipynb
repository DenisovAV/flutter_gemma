{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "header"},
   "source": ["# FunctionGemma Base → TFLite Conversion\n\nConverts the base FunctionGemma model to TFLite format (no fine-tuning).\n\n**Pipeline (Base Model - No Fine-tuning):**\n1. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_base_download.ipynb) [functiongemma_base_download.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_base_download.ipynb) - Download base model ✅\n2. **This notebook** - Convert to TFLite\n3. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_base_tflite_to_task.ipynb) [functiongemma_base_tflite_to_task.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_base_tflite_to_task.ipynb) - Bundle as .task for Flutter\n\n**Requirements:**\n- A100 or L4 GPU runtime\n- Base model on Google Drive (from Step 1)\n\n**Output:** `.tflite` file saved to Google Drive"]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "install_header"},
   "source": ["## Step 1: Install Dependencies\n\n**RESTART RUNTIME** after running this cell!"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "install"},
   "outputs": [],
   "source": ["!pip uninstall -y tensorflow 2>/dev/null || true\n\n!pip install ai-edge-torch --force-reinstall -q\n!pip install \"numpy<2.1\" --force-reinstall -q\n!pip install transformers==4.57.3 huggingface_hub sentencepiece -q\n!pip install Pillow --force-reinstall -q\n\nprint(\"\\nInstalled:\")\n!pip show ai-edge-torch | grep Version\n!pip show numpy | grep Version\n\nprint(\"\\n RESTART RUNTIME after this step! (Runtime -> Restart session)\")"]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "load_header"},
   "source": ["## Step 2: Load Model from Google Drive"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "load"},
   "outputs": [],
   "source": ["from google.colab import drive\nimport os\n\ndrive.mount('/content/drive')\n\nMODEL_NAME = \"functiongemma-base\"\nMODEL_DIR = MODEL_NAME\nDRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{MODEL_NAME}\"\n\nif os.path.exists(DRIVE_MODEL_DIR):\n    print(f\"Found folder: {DRIVE_MODEL_DIR}\")\n    !cp -r \"{DRIVE_MODEL_DIR}\" .\nelse:\n    raise FileNotFoundError(f\"Model not found!\\nRun functiongemma_base_download.ipynb first!\")\n\nprint(f\"\\nModel ready:\")\n!ls -la \"{MODEL_DIR}/\""]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "test_header"},
   "source": ["## Step 3: Test Model Before Conversion"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "test"},
   "outputs": [],
   "source": ["import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nprint(f\"Loading model from {MODEL_DIR}...\")\n\nhf_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_DIR,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    attn_implementation=\"eager\"\n)\nhf_model.eval()\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\ntest_prompt = \"\"\"<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:change_background_color{description:<escape>Changes the app background color<escape>,parameters:{properties:{color:{description:<escape>The color name (red, green, blue, yellow, purple, orange)<escape>,type:<escape>STRING<escape>}},required:[<escape>color<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTING BASE MODEL\")\nprint(\"=\" * 50)\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(hf_model.device)\n\nwith torch.no_grad():\n    outputs = hf_model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(f\"Input: 'make it red'\")\nprint(f\"Output: {response}\")\nprint(\"=\" * 50)\n\ndel hf_model\ntorch.cuda.empty_cache()"]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "convert_header"},
   "source": ["## Step 4: Convert to TFLite\n\n**Time:** ~5-10 min on A100"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "convert"},
   "outputs": [],
   "source": ["import os\nfrom ai_edge_torch.generative.examples.gemma3 import gemma3\nfrom ai_edge_torch.generative.utilities import converter\nfrom ai_edge_torch.generative.utilities.export_config import ExportConfig\nfrom ai_edge_torch.generative.layers import kv_cache\n\nTFLITE_OUTPUT_DIR = \"tflite_output\"\nos.makedirs(TFLITE_OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Loading model via ai-edge-torch...\")\npytorch_model = gemma3.build_model_270m(MODEL_DIR)\npytorch_model.eval()\nprint(\"Model loaded!\")\n\nexport_config = ExportConfig()\nexport_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\nexport_config.mask_as_input = True\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Converting to TFLite...\")\nprint(\"=\" * 50)\n\nconverter.convert_to_tflite(\n    pytorch_model,\n    output_path=TFLITE_OUTPUT_DIR,\n    output_name_prefix=\"functiongemma-base\",\n    prefill_seq_len=256,\n    kv_cache_max_len=1024,\n    quantize=\"dynamic_int8\",\n    export_config=export_config,\n)\n\nprint(\"\\n Conversion complete!\")\n!ls -lah {TFLITE_OUTPUT_DIR}/"]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "save_header"},
   "source": ["## Step 5: Save to Google Drive"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "save"},
   "outputs": [],
   "source": ["import glob\nimport shutil\n\nDRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\nos.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n\n# Save TFLite file\ntflite_files = glob.glob(f\"{TFLITE_OUTPUT_DIR}/*.tflite\")\nfor f in tflite_files:\n    size = os.path.getsize(f) / 1e6\n    dest = f\"{DRIVE_OUTPUT_DIR}/{os.path.basename(f)}\"\n    shutil.copy(f, dest)\n    print(f\"Saved: {dest} ({size:.1f} MB)\")\n\n# Save tokenizer\ntokenizer_src = f\"{MODEL_DIR}/tokenizer.model\"\nif os.path.exists(tokenizer_src):\n    tokenizer_dest = f\"{DRIVE_OUTPUT_DIR}/tokenizer.model\"\n    shutil.copy(tokenizer_src, tokenizer_dest)\n    print(f\"Saved: {tokenizer_dest}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"DONE!\")\nprint(\"=\" * 50)\nprint(\"\\nNext: Run functiongemma_base_tflite_to_task.ipynb\")"]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {"gpuType": "A100", "provenance": []},
  "kernelspec": {"display_name": "Python 3", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
