{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FunctionGemma Load Test\n\nTests loading fine-tuned model from Google Drive in a fresh session.\n\n**⚠️ CRITICAL Loading Parameters:**\n- `torch_dtype=torch.bfloat16` (NOT float16!)\n- `attn_implementation=\"eager\"`\n\nThese must match training parameters, otherwise model outputs garbage.\n\n**Requirements:**\n- GPU runtime (T4 or better)\n- Model ZIP on Google Drive (`functiongemma-flutter-demo-final.zip`)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies (SAME versions as finetuning!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Use same transformers version as finetuning notebook!\n",
    "!pip install transformers==4.57.3 -q\n",
    "!pip install sentencepiece -q\n",
    "\n",
    "import transformers\n",
    "print(f\"transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Drive & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_DIR = \"functiongemma-flutter-demo-final\"\n",
    "DRIVE_ZIP = f\"/content/drive/MyDrive/{MODEL_DIR}.zip\"\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    !unzip -q \"{DRIVE_ZIP}\"\n",
    "    print(f\"Extracted: {os.listdir(MODEL_DIR)}\")\n",
    "else:\n",
    "    print(f\"Already exists: {os.listdir(MODEL_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model & Check Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Check GPU availability\nprint(\"GPU Check:\")\nprint(f\"   CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nprint(f\"\\nLoading model from {MODEL_DIR}...\")\n\n# CRITICAL: Must match training parameters!\n# - bfloat16 (NOT float16!)\n# - attn_implementation=\"eager\"\nif torch.cuda.is_available():\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_DIR,\n        torch_dtype=torch.bfloat16,       # CRITICAL: same as training!\n        device_map=\"cuda:0\",\n        attn_implementation=\"eager\"        # CRITICAL: same as training!\n    )\nelse:\n    print(\"⚠️ WARNING: No GPU - model may not work correctly\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_DIR,\n        torch_dtype=torch.float32,\n        device_map=\"cpu\"\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\nprint(f\"\\n✅ Model loaded on: {model.device}\")\nprint(f\"   dtype: {model.dtype}\")\nprint(f\"\\nConfig:\")\nprint(f\"   model.config.pad_token_id = {model.config.pad_token_id}\")\nprint(f\"   model.config.eos_token_id = {model.config.eos_token_id}\")\nprint(f\"   model.config.bos_token_id = {model.config.bos_token_id}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define tool\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"change_background_color\",\n        \"description\": \"Changes the background color\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"color\": {\"type\": \"string\", \"description\": \"Color name\"}\n            },\n            \"required\": [\"color\"]\n        }\n    }\n}]\n\ntest_prompts = [\n    \"make it red\",\n    \"change background to blue\",\n    \"I want purple\",\n]\n\nprint(\"Testing model:\")\nprint(f\"Device: {model.device}\")\nprint(\"=\" * 60)\n\nall_passed = True\n\nfor prompt in test_prompts:\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    input_text = tokenizer.apply_chat_template(\n        messages,\n        tools=tools,\n        add_generation_prompt=True,\n        tokenize=False\n    )\n    \n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=80,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    \n    response = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[1]:],\n        skip_special_tokens=False\n    )\n    \n    # Check result\n    is_valid = \"<start_function_call>\" in response and \"<pad>\" not in response[:50]\n    status = \"✅\" if is_valid else \"❌\"\n    if not is_valid:\n        all_passed = False\n    \n    print(f\"\\n[{status}] User: {prompt}\")\n    print(f\"      Model: {response.strip()[:80]}...\")\n    print(\"-\" * 60)\n\nprint(\"\\n\" + \"=\" * 60)\nif all_passed:\n    print(\"✅ RESULT: ALL TESTS PASSED!\")\n    print(\"Model works correctly after save/load.\")\nelse:\n    print(\"❌ RESULT: TESTS FAILED!\")\n    if str(model.device) == \"cpu\":\n        print(\"⚠️  Model is on CPU - this may be the cause!\")\n        print(\"   Try: Runtime → Change runtime type → T4 GPU\")\n    else:\n        print(\"   Model is on GPU but still fails - investigate tokenizer.\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Version Info (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "print(\"Environment:\")\n",
    "print(f\"   Python: {sys.version}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Transformers: {transformers.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  }
 ]
}