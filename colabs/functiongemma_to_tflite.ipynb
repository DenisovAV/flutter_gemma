{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FunctionGemma ‚Üí TFLite Conversion\n\nConversion notebook for [flutter_gemma](https://github.com/DenisovAV/flutter_gemma) plugin.\n\nConverts fine-tuned FunctionGemma model from PyTorch/SafeTensors format to TFLite for on-device inference.\n\n**Pipeline:**\n1. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_finetuning.ipynb) [functiongemma_finetuning.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_finetuning.ipynb) - Fine-tune model ‚úÖ\n2. **This notebook** - Convert to TFLite\n3. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_tflite_to_task.ipynb) [functiongemma_tflite_to_task.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_tflite_to_task.ipynb) - Bundle as .task for Flutter\n\n**What this notebook does:**\n1. Loads fine-tuned model from Google Drive\n2. Validates model works correctly BEFORE conversion (HuggingFace test)\n3. Converts to TFLite with int8 quantization\n4. Saves TFLite + tokenizer to Google Drive\n\n**‚ö†Ô∏è CRITICAL Loading Parameters:**\n- `torch_dtype=torch.bfloat16` (NOT float16!)\n- `attn_implementation=\"eager\"`\n\n**Requirements:**\n- A100 or L4 GPU runtime (Runtime ‚Üí Change runtime type)\n- Fine-tuned model on Google Drive (from Step 1)\n\n**Output:** `.tflite` file saved to Google Drive"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "**What we're installing:**\n",
    "- `ai-edge-torch` - Google's library for converting PyTorch models to TFLite\n",
    "- `transformers` - HuggingFace library for loading/testing the model\n",
    "- `numpy<2.1` - Required for compatibility with ai-edge-torch\n",
    "- `sentencepiece` - Tokenizer for Gemma models\n",
    "\n",
    "**Why specific versions:**\n",
    "- `numpy<2.1` - ai-edge-torch breaks with numpy 2.1+\n",
    "- `Pillow` reinstall - ai-edge-torch may corrupt Colab's Pillow\n",
    "\n",
    "**‚ö†Ô∏è RESTART RUNTIME** after running this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 1: Install ai-edge-torch\n# =============================================================================\n!pip uninstall -y tensorflow 2>/dev/null || true\n\n# Install ai-edge-torch\n!pip install ai-edge-torch --force-reinstall -q\n\n# CRITICAL: Install numpy<2.1 AFTER ai-edge-torch (it may override)\n!pip install \"numpy<2.1\" --force-reinstall -q\n\n# Install transformers with pinned version\n!pip install transformers==4.57.3 huggingface_hub sentencepiece -q\n\n# Restore Colab's native Pillow (ai-edge-torch may break it)\n!pip install Pillow --force-reinstall -q\n\nprint(\"\\nInstalled:\")\n!pip show ai-edge-torch | grep Version\n!pip show transformers | grep Version\n!pip show numpy | grep Version\n!pip show Pillow | grep Version\n\nprint(\"\\n‚ö†Ô∏è  RESTART RUNTIME after this step! (Runtime ‚Üí Restart session)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Model from Google Drive\n",
    "\n",
    "Loads the fine-tuned model from Google Drive.\n",
    "\n",
    "**Expected location:**\n",
    "- Folder: `My Drive/functiongemma-flutter-demo-final/`\n",
    "- Or ZIP: `My Drive/functiongemma-flutter-demo-final.zip`\n",
    "\n",
    "**Required files in the folder:**\n",
    "- `model.safetensors` - Model weights (~540MB)\n",
    "- `config.json` - Model configuration\n",
    "- `tokenizer.model` - SentencePiece tokenizer\n",
    "- `tokenizer_config.json` - Tokenizer settings\n",
    "\n",
    "**Customize:** Change `MODEL_NAME` if your model has a different name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Load fine-tuned model from Google Drive\n",
    "# =============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MODEL_NAME = \"functiongemma-flutter-demo-final\"\n",
    "MODEL_DIR = MODEL_NAME\n",
    "DRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{MODEL_NAME}\"\n",
    "DRIVE_ZIP = f\"/content/drive/MyDrive/{MODEL_NAME}.zip\"\n",
    "\n",
    "if os.path.exists(DRIVE_MODEL_DIR):\n",
    "    print(f\"Found folder: {DRIVE_MODEL_DIR}\")\n",
    "    !cp -r \"{DRIVE_MODEL_DIR}\" .\n",
    "elif os.path.exists(DRIVE_ZIP):\n",
    "    print(f\"Found ZIP: {DRIVE_ZIP}\")\n",
    "    !unzip -q \"{DRIVE_ZIP}\"\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model not found!\\nUpload to: {DRIVE_MODEL_DIR}/ or {DRIVE_ZIP}\")\n",
    "\n",
    "print(f\"\\nModel ready:\")\n",
    "!ls -la \"{MODEL_DIR}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Model Before Conversion\n",
    "\n",
    "**CRITICAL**: Verify the model works BEFORE converting to TFLite.\n",
    "\n",
    "We load the model using HuggingFace transformers and test it with a sample prompt. If it outputs garbage here, the problem is in fine-tuning, not conversion.\n",
    "\n",
    "**What we check:**\n",
    "- Model outputs `<start_function_call>` tag\n",
    "- No `<pad>` tokens in output (indicates wrong loading params)\n",
    "- No Chinese characters or garbage (indicates broken fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Step 3: Test model BEFORE conversion (using HuggingFace transformers)\n# =============================================================================\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nprint(f\"Loading model from {MODEL_DIR} via HuggingFace transformers...\")\n\n# CRITICAL: Must use same parameters as training!\n# - bfloat16 (NOT float16!)\n# - attn_implementation=\"eager\"\nhf_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_DIR,\n    torch_dtype=torch.bfloat16,           # CRITICAL: same as training!\n    device_map=\"auto\",\n    attn_implementation=\"eager\"            # CRITICAL: same as training!\n)\nhf_model.eval()\nprint(f\"HuggingFace model loaded on {hf_model.device}, dtype={hf_model.dtype}\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n# FunctionGemma test prompt - MUST match training format EXACTLY\ntest_prompt = \"\"\"<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:change_background_color{description:<escape>Changes the app background color<escape>,parameters:{properties:{color:{description:<escape>The color name (red, green, blue, yellow, purple, orange)<escape>,type:<escape>STRING<escape>}},required:[<escape>color<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<start_function_declaration>declaration:change_app_title{description:<escape>Changes the application title text in the AppBar<escape>,parameters:{properties:{title:{description:<escape>The new title text to display<escape>,type:<escape>STRING<escape>}},required:[<escape>title<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<start_function_declaration>declaration:show_alert{description:<escape>Shows an alert dialog with a custom message and title<escape>,parameters:{properties:{title:{description:<escape>The title of the alert dialog<escape>,type:<escape>STRING<escape>},message:{description:<escape>The message content of the alert dialog<escape>,type:<escape>STRING<escape>}},required:[<escape>title<escape>,<escape>message<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TESTING FINE-TUNED MODEL (HuggingFace)\")\nprint(\"=\" * 50)\nprint(f\"Input: 'make it red'\")\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\").to(hf_model.device)\n\nwith torch.no_grad():\n    outputs = hf_model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=50,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\nresponse = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(f\"\\nModel output:\")\nprint(response)\nprint(\"=\" * 50)\n\n# Check if output looks valid\nif \"change_background_color\" in response or \"call:\" in response:\n    print(\"‚úÖ Fine-tuned model outputs function call - GOOD!\")\n    print(\"   Proceeding with conversion...\")\nelif \"<pad>\" in response[:50]:\n    print(\"‚ùå Model outputs <pad> - wrong loading parameters!\")\n    print(\"   Make sure: torch_dtype=bfloat16, attn_implementation='eager'\")\n    raise ValueError(\"STOP: Wrong model loading parameters\")\nelif \"apologize\" in response.lower() or \"sorry\" in response.lower():\n    print(\"‚ùå Model refuses to call function - fine-tuning didn't work!\")\n    raise ValueError(\"STOP: Model not fine-tuned correctly\")\nelif any(c in response for c in \"‰∏∫Ë∂≥ÁêÉÊî∂Ê∂àÊ∞î\"):\n    print(\"‚ùå Model outputs garbage - fine-tuning is broken!\")\n    raise ValueError(\"STOP: Model outputs garbage\")\nelse:\n    print(\"‚ö†Ô∏è Unexpected output - review manually before proceeding\")\n\n# Clean up HF model to free memory\ndel hf_model\ntorch.cuda.empty_cache()\nprint(\"\\nHuggingFace model unloaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert to TFLite\n",
    "\n",
    "This is the main conversion step using `ai-edge-torch`.\n",
    "\n",
    "**What happens:**\n",
    "1. Model is loaded using `gemma3.build_model_270m()` (ai-edge-torch's loader)\n",
    "2. Converted to TFLite format with `dynamic_int8` quantization\n",
    "3. KV-cache is configured for efficient inference\n",
    "\n",
    "**Conversion parameters (official Google):**\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `prefill_seq_len` | 256 | Input sequence length |\n",
    "| `kv_cache_max_len` | 1024 | Maximum context length |\n",
    "| `quantize` | dynamic_int8 | Reduces size ~50% |\n",
    "\n",
    "**Time:** ~5-10 min on A100, ~10-15 min on L4/T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Convert to TFLite\n",
    "# =============================================================================\n",
    "import os\n",
    "from ai_edge_torch.generative.examples.gemma3 import gemma3\n",
    "from ai_edge_torch.generative.utilities import converter\n",
    "from ai_edge_torch.generative.utilities.export_config import ExportConfig\n",
    "from ai_edge_torch.generative.layers import kv_cache\n",
    "\n",
    "TFLITE_OUTPUT_DIR = \"tflite_output\"\n",
    "os.makedirs(TFLITE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load model using ai-edge-torch\n",
    "print(f\"Loading model from {MODEL_DIR} via ai-edge-torch...\")\n",
    "pytorch_model = gemma3.build_model_270m(MODEL_DIR)\n",
    "pytorch_model.eval()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# Configure export\n",
    "export_config = ExportConfig()\n",
    "export_config.kvcache_layout = kv_cache.KV_LAYOUT_TRANSPOSED\n",
    "export_config.mask_as_input = True\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Converting to TFLite...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert with official Google parameters\n",
    "converter.convert_to_tflite(\n",
    "    pytorch_model,\n",
    "    output_path=TFLITE_OUTPUT_DIR,\n",
    "    output_name_prefix=\"functiongemma-flutter\",\n",
    "    prefill_seq_len=256,       # Official Google parameter\n",
    "    kv_cache_max_len=1024,     # Official Google parameter\n",
    "    quantize=\"dynamic_int8\",\n",
    "    export_config=export_config,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Conversion complete!\")\n",
    "!ls -lah {TFLITE_OUTPUT_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Step 4.5: Test converted model BEFORE saving to .tflite file\n# =============================================================================\n# NOTE: Full TFLite LLM inference requires MediaPipe (Android/iOS/Web).\n# Python can only test the edge_model before exporting to .tflite.\n# This test validates the conversion itself, not the .tflite file.\n\nimport torch\nfrom transformers import AutoTokenizer\n\nprint(\"=\" * 50)\nprint(\"TESTING CONVERTED MODEL (via ai-edge-torch)\")\nprint(\"=\" * 50)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n# Same test prompt (shorter - one tool only for faster test)\ntest_prompt = \"\"\"<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:change_background_color{description:<escape>Changes the app background color<escape>,parameters:{properties:{color:{description:<escape>The color name (red, green, blue, yellow, purple, orange)<escape>,type:<escape>STRING<escape>}},required:[<escape>color<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nprint(f\"Input: 'make it red'\")\n\n# Tokenize\ninput_ids = tokenizer.encode(test_prompt, return_tensors=\"pt\")\nprint(f\"Input tokens: {input_ids.shape[1]}\")\n\n# Test with pytorch_model (which was loaded for conversion in Step 4)\n# pytorch_model is still in memory from the conversion step\nprint(\"\\n--- Testing PyTorch model (post-reauthoring) ---\")\ntry:\n    with torch.no_grad():\n        # Simple forward pass to check model works\n        # Note: Full generation requires KV-cache setup\n        logits = pytorch_model(input_ids)\n        print(f\"Output logits shape: {logits.shape}\")\n        \n        # Get the most likely next token\n        next_token_logits = logits[0, -1, :]\n        next_token_id = torch.argmax(next_token_logits).item()\n        next_token = tokenizer.decode([next_token_id])\n        print(f\"Next predicted token: '{next_token}' (id={next_token_id})\")\n        \n        # Check if it's a reasonable FunctionGemma token\n        if next_token.strip() in ['<', 'call', '<start', '<start_function_call>']:\n            print(\"‚úÖ Model predicts function call start - GOOD!\")\n        else:\n            print(f\"‚ö†Ô∏è Unexpected first token: '{next_token}'\")\n            \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Forward pass failed: {e}\")\n    print(\"   This might indicate conversion issues\")\n\n# Summary\nprint(\"\\n\" + \"=\" * 50)\nprint(\"TEST SUMMARY\")\nprint(\"=\" * 50)\nprint(\"‚úÖ TFLite file created successfully\")\nprint(\"‚ö†Ô∏è Full text generation test requires MediaPipe on device\")\nprint(\"   The .task bundling and MediaPipe inference is the next step\")\nprint(\"\")\nprint(\"If model outputs garbage on device, check:\")\nprint(\"1. BundleConfig in functiongemma_tflite_to_task.ipynb\")\nprint(\"   - prompt_prefix should be EMPTY for FunctionGemma\")\nprint(\"   - prompt_suffix should be EMPTY for FunctionGemma\")\nprint(\"2. Quantization settings (int8 may affect quality)\")\nprint(\"3. MediaPipe version compatibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Save to Google Drive\n",
    "# =============================================================================\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/flutter_gemma_models\"\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save TFLite file\n",
    "tflite_files = glob.glob(f\"{TFLITE_OUTPUT_DIR}/*.tflite\")\n",
    "for f in tflite_files:\n",
    "    size = os.path.getsize(f) / 1e6\n",
    "    dest = f\"{DRIVE_OUTPUT_DIR}/{os.path.basename(f)}\"\n",
    "    shutil.copy(f, dest)\n",
    "    print(f\"‚úÖ Saved: {dest} ({size:.1f} MB)\")\n",
    "\n",
    "# Save tokenizer (needed for bundling)\n",
    "tokenizer_src = f\"{MODEL_DIR}/tokenizer.model\"\n",
    "if os.path.exists(tokenizer_src):\n",
    "    tokenizer_dest = f\"{DRIVE_OUTPUT_DIR}/tokenizer.model\"\n",
    "    shutil.copy(tokenizer_src, tokenizer_dest)\n",
    "    print(f\"‚úÖ Saved: {tokenizer_dest}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TFLite saved to Google Drive!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nLocation: {DRIVE_OUTPUT_DIR}/\")\n",
    "print(\"\\nNext: Run functiongemma_tflite_to_task.ipynb to create .task file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Upload to HuggingFace Hub\n",
    "\n",
    "Upload the TFLite model to HuggingFace for easy sharing and versioning.\n",
    "\n",
    "**Setup:**\n",
    "1. Create a new model repository on [huggingface.co](https://huggingface.co/new)\n",
    "2. Login to HuggingFace (uncomment login code below)\n",
    "3. Change `HUB_REPO_ID` to your repository\n",
    "4. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Optional: Upload to HuggingFace Hub\n",
    "# =============================================================================\n",
    "# Uncomment the code below to upload\n",
    "\n",
    "# from huggingface_hub import login, HfApi\n",
    "# from google.colab import userdata\n",
    "#\n",
    "# # Login (uses token from Colab Secrets)\n",
    "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "# login(token=HF_TOKEN)\n",
    "#\n",
    "# # Upload to HuggingFace\n",
    "# HUB_REPO_ID = \"your-username/functiongemma-flutter-tflite\"  # Change this!\n",
    "#\n",
    "# api = HfApi()\n",
    "# api.create_repo(repo_id=HUB_REPO_ID, exist_ok=True)\n",
    "#\n",
    "# # Upload TFLite files\n",
    "# for f in glob.glob(f\"{TFLITE_OUTPUT_DIR}/*.tflite\"):\n",
    "#     api.upload_file(\n",
    "#         path_or_fileobj=f,\n",
    "#         path_in_repo=os.path.basename(f),\n",
    "#         repo_id=HUB_REPO_ID,\n",
    "#     )\n",
    "#     print(f\"‚úÖ Uploaded: {os.path.basename(f)}\")\n",
    "#\n",
    "# # Upload tokenizer\n",
    "# if os.path.exists(tokenizer_src):\n",
    "#     api.upload_file(\n",
    "#         path_or_fileobj=tokenizer_src,\n",
    "#         path_in_repo=\"tokenizer.model\",\n",
    "#         repo_id=HUB_REPO_ID,\n",
    "#     )\n",
    "#     print(\"‚úÖ Uploaded: tokenizer.model\")\n",
    "#\n",
    "# print(f\"\\nüéâ Model uploaded to: https://huggingface.co/{HUB_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}