{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# FunctionGemma Fine-tuning for Flutter\n\nFine-tuning notebook for [flutter_gemma](https://github.com/AdenisovAV/flutter_gemma) plugin.\n\nTrains FunctionGemma 270M for custom function calling:\n- `change_background_color` - Changes app background color\n- `change_app_title` - Changes app title\n- `show_alert` - Shows alert dialog\n\n**Pipeline:**\n1. **This notebook** - Fine-tune model\n2. `functiongemma_to_tflite.ipynb` - Convert to TFLite\n3. `functiongemma_tflite_to_task.ipynb` - Bundle as .task for Flutter\n\n**Requirements:**\n- A100 GPU runtime (Runtime â†’ Change runtime type â†’ A100)\n- HuggingFace account with accepted [Gemma license](https://huggingface.co/google/functiongemma-270m-it)\n- HuggingFace token with write access",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Install Dependencies\n\n**What we're installing:**\n- `torch` - PyTorch, the core ML framework\n- `transformers` - HuggingFace library for working with LLMs\n- `trl` - Transformer Reinforcement Learning, for SFT (Supervised Fine-Tuning)\n- `datasets` - for dataset handling\n- `accelerate` - for distributed training and GPU optimization\n- `sentencepiece` - tokenizer for Gemma models\n\n**Versions are pinned** for reproducibility.",
   "metadata": {
    "id": "install_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Install dependencies (use Colab's pre-installed torch)\n# =============================================================================\n# Don't reinstall torch - Colab has optimized version pre-installed\n\n!pip install -q transformers==4.57.3 datasets accelerate evaluate trl==0.26.2 protobuf sentencepiece\n!pip install -q huggingface_hub tensorboard\n\nprint(\"\\nDependencies installed!\")",
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. HuggingFace Authentication\n\n**Setup Colab Secret:**\n1. Click the ðŸ”‘ key icon in the left panel\n2. Add new secret: `HF_TOKEN`\n3. Paste your HuggingFace token (from https://huggingface.co/settings/tokens)\n4. Toggle \"Notebook access\" ON\n\n**Don't forget:** Accept the Gemma license at https://huggingface.co/google/functiongemma-270m-it",
   "metadata": {
    "id": "auth_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Authenticate with HuggingFace using Colab Secrets\n# =============================================================================\nfrom google.colab import userdata\nfrom huggingface_hub import login\n\n# Read token from Colab Secrets (key icon in left panel)\nHF_TOKEN = userdata.get('HF_TOKEN')\n\nif not HF_TOKEN:\n    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Add it via the ðŸ”‘ key icon.\")\n\nlogin(token=HF_TOKEN)\nprint(\"Logged in to HuggingFace!\")",
   "metadata": {
    "id": "auth"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Upload Training Data\n\n**File:** `training_data.jsonl` (284 examples)\n\n**Format per line:**\n```json\n{\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n```\n\n**How to upload:**\n1. Drag and drop the file into the left panel (Files)\n2. Or run this cell - an Upload button will appear",
   "metadata": {
    "id": "upload_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "if not os.path.exists('training_data.jsonl'):\n",
    "    print(\"Please upload training_data.jsonl:\")\n",
    "    uploaded = files.upload()\n",
    "else:\n",
    "    print(\"âœ… training_data.jsonl already exists\")\n",
    "\n",
    "# Verify file\n",
    "!wc -l training_data.jsonl\n",
    "!head -1 training_data.jsonl"
   ],
   "metadata": {
    "id": "upload_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Define Tools and Prepare Dataset\n\n### 4.1 Define Functions (Tools)\n\nWe define Python functions with type hints and docstrings. The `get_json_schema()` utility from HuggingFace transformers automatically generates JSON Schema for each function.\n\n**Why this approach:**\n- Type hints â†’ parameter types in schema\n- Docstrings â†’ function descriptions\n- No manual JSON writing needed",
   "metadata": {
    "id": "prepare_header"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datasets import Dataset\nfrom transformers.utils import get_json_schema\n\n# =============================================================================\n# STEP 4.1: Define Python functions for JSON Schema generation\n# =============================================================================\n# These functions are NOT executed - they're only used for JSON Schema generation.\n# get_json_schema() reads the function name, docstring, and type hints,\n# and creates a JSON Schema in OpenAI function calling format.\n\ndef change_background_color(color: str) -> str:\n    \"\"\"Changes the app background color to specified color.\n\n    Args:\n        color: The color name (red, green, blue, yellow, purple, orange)\n    \"\"\"\n    return f\"Changed to {color}\"\n\ndef change_app_title(title: str) -> str:\n    \"\"\"Changes the application title text in the AppBar.\n\n    Args:\n        title: The new title text to display\n    \"\"\"\n    return f\"Title set to {title}\"\n\ndef show_alert(title: str, message: str) -> str:\n    \"\"\"Shows an alert dialog with a custom message and title.\n\n    Args:\n        title: The title of the alert dialog\n        message: The message content of the alert dialog\n    \"\"\"\n    return f\"Alert shown: {title}\"\n\n# =============================================================================\n# Generate JSON Schemas from Python functions\n# =============================================================================\n# get_json_schema() creates a structure like:\n# {\"type\": \"function\", \"function\": {\"name\": \"...\", \"description\": \"...\", \"parameters\": {...}}}\n\nTOOLS = [\n    get_json_schema(change_background_color),\n    get_json_schema(change_app_title),\n    get_json_schema(show_alert),\n]\n\nprint(\"Tools defined:\")\nfor tool in TOOLS:\n    print(f\"   - {tool['function']['name']}: {tool['function']['description'][:50]}...\")",
   "metadata": {
    "id": "define_tools"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Convert Dataset\n\nOur simple training format is converted to FunctionGemma conversation format:\n\n```\nBefore (our format):\n{\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n\nAfter (FunctionGemma format):\n{\n  \"messages\": [\n    {\"role\": \"developer\", \"content\": \"You are a model that can do function calling with the following functions\"},\n    {\"role\": \"user\", \"content\": \"make it red\"},\n    {\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": {\"name\": \"...\", \"arguments\": {...}}}]}\n  ],\n  \"tools\": [... JSON schemas ...]\n}\n```\n\n**Key points:**\n- `developer` role is required (not `system`) - this activates function calling mode\n- `tools` array contains JSON schemas for available functions\n- `tool_calls` in assistant response shows expected output",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# STEP 4.2: Load and convert dataset to FunctionGemma format\n# =============================================================================\n\n# System message for developer role (official Google format)\n# Source: https://huggingface.co/google/functiongemma-270m-it\nDEFAULT_SYSTEM_MSG = \"You are a model that can do function calling with the following functions\"\n\ndef create_conversation(sample):\n    \"\"\"\n    Converts simple format to full FunctionGemma conversation format.\n    \n    Input (our simple format):\n        {\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n    \n    Output (FunctionGemma format):\n        {\n            \"messages\": [\n                {\"role\": \"developer\", \"content\": \"You are a model...\"},  # System prompt\n                {\"role\": \"user\", \"content\": \"make it red\"},              # User request\n                {\"role\": \"assistant\", \"tool_calls\": [                    # Expected response\n                    {\"type\": \"function\", \"function\": {\"name\": \"change_background_color\", \"arguments\": {\"color\": \"red\"}}}\n                ]}\n            ],\n            \"tools\": [... JSON Schemas ...]  # Available tools\n        }\n    \"\"\"\n    return {\n        \"messages\": [\n            {\"role\": \"developer\", \"content\": DEFAULT_SYSTEM_MSG},\n            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n            {\n                \"role\": \"assistant\",\n                \"tool_calls\": [\n                    {\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": sample[\"tool_name\"],\n                            \"arguments\": json.loads(sample[\"tool_arguments\"])  # Parse JSON string to dict\n                        }\n                    }\n                ]\n            },\n        ],\n        \"tools\": TOOLS\n    }\n\n# =============================================================================\n# Load raw JSONL data\n# =============================================================================\nraw_data = []\nwith open('training_data.jsonl', 'r') as f:\n    for line in f:\n        raw_data.append(json.loads(line.strip()))\n\nprint(f\"Loaded {len(raw_data)} raw examples\")\n\n# =============================================================================\n# Convert to HuggingFace Dataset and apply transformation\n# =============================================================================\ndataset = Dataset.from_list(raw_data)\ndataset = dataset.map(create_conversation, remove_columns=dataset.features)\n\n# Split into train/test (90%/10%)\ndataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n\nprint(f\"Dataset prepared:\")\nprint(f\"   Train: {len(dataset['train'])} examples\")\nprint(f\"   Test:  {len(dataset['test'])} examples\")\nprint(f\"\\nSample conversation (first example):\")\nprint(json.dumps(dataset['train'][0], indent=2)[:500] + \"...\")",
   "metadata": {
    "id": "load_dataset"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load Base Model\n\n**Model:** `google/functiongemma-270m-it`\n- 270M parameters (compact, designed for on-device)\n- Instruction-tuned (it) - already trained to follow instructions\n- Specialized for function calling\n\n**Loading parameters:**\n- `torch_dtype=bfloat16` - 16-bit weights to save memory (~540MB instead of ~1GB)\n- `device_map=\"auto\"` - automatically load to GPU\n- `attn_implementation=\"eager\"` - without FlashAttention (for compatibility)",
   "metadata": {
    "id": "model_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# =============================================================================\n# Load FunctionGemma base model\n# =============================================================================\nBASE_MODEL = \"google/functiongemma-270m-it\"\n\nprint(f\"Loading {BASE_MODEL}...\")\nprint(\"   (Downloads ~540MB on first run, then uses cache)\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.bfloat16,      # 16-bit to save VRAM\n    device_map=\"auto\",                # Automatically load to GPU\n    attn_implementation=\"eager\"       # Without FlashAttention for compatibility\n)\n\n# Tokenizer converts text to tokens and back\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nprint(f\"\\nModel loaded!\")\nprint(f\"   Parameters: {model.num_parameters():,}\")\nprint(f\"   Memory: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")\nprint(f\"   Device: {model.device}\")",
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Configure Training\n\n**Hyperparameters from official Google FunctionGemma cookbook:**\n\n| Parameter | Value | Explanation |\n|-----------|-------|-------------|\n| `num_train_epochs` | 3 | How many times to iterate through the entire dataset |\n| `learning_rate` | 1e-5 | Learning rate (conservative for fine-tuning) |\n| `lr_scheduler_type` | cosine | Smoothly decreases LR towards end of training |\n| `gradient_accumulation_steps` | 8 | Gradient accumulation (effective batch = 32) |\n| `max_length` | 1024 | Maximum sequence length in tokens |\n| `bf16` | True | 16-bit training to save memory |\n\n**Why these values:**\n- LR 1e-5 (not 5e-5) - prevents \"forgetting\" base knowledge\n- Cosine scheduler - smooth LR decay improves convergence\n- Gradient accumulation 8 - simulates large batch without running out of memory",
   "metadata": {
    "id": "config_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from trl import SFTConfig, SFTTrainer\n\n# Output directory\nOUTPUT_DIR = \"functiongemma-flutter-demo\"\n\n# =============================================================================\n# Training configuration (based on official Google FunctionGemma cookbook)\n# https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n# =============================================================================\ntraining_args = SFTConfig(\n    output_dir=OUTPUT_DIR,\n    \n    # Training params (Google official uses 2 epochs, we use 3 for small dataset)\n    max_length=1024,                    # Max sequence length in tokens\n    packing=False,                      # Don't pack multiple examples into one sequence\n    num_train_epochs=3,                 # Google uses 2, we add 1 for small dataset (284 examples)\n    per_device_train_batch_size=4,      # Batch size per GPU\n    per_device_eval_batch_size=4,       # Eval batch size\n    gradient_accumulation_steps=8,      # Effective batch size: 4 * 8 = 32\n    \n    # Optimizer (Google official params)\n    learning_rate=1e-5,                 # Google official: 1e-5 (more conservative than 5e-5)\n    lr_scheduler_type=\"cosine\",         # Google official: cosine decay\n    optim=\"adamw_torch_fused\",          # Fused AdamW for faster training\n    warmup_ratio=0.1,                   # 10% warmup steps\n    \n    # Logging and checkpoints\n    logging_steps=10,                   # Log every 10 steps\n    eval_strategy=\"epoch\",              # Evaluate after each epoch\n    save_strategy=\"epoch\",              # Save checkpoint after each epoch\n    \n    # Memory optimization\n    gradient_checkpointing=False,       # Trade compute for memory (enable if OOM)\n    bf16=True,                          # Use bfloat16 for training\n    \n    # Output\n    report_to=\"tensorboard\",            # Log to TensorBoard\n    push_to_hub=False,                  # Set to True to upload to HuggingFace\n)\n\nprint(\"Training configuration (Google official params):\")\nprint(f\"   Epochs: {training_args.num_train_epochs}\")\nprint(f\"   Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\nprint(f\"   LR scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"   Max length: {training_args.max_length}\")",
   "metadata": {
    "id": "config_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Start Training\n\n**What happens:**\n1. `SFTTrainer` applies `tokenizer.apply_chat_template()` to each example\n2. Model learns to predict the correct `tool_call` for each `user_content`\n3. Loss is calculated only on assistant responses (not on prompts)\n\n**Training time:** ~5 minutes on A100 GPU (for ~300 examples)\n\n**Monitoring:**\n- `loss` should decrease\n- `eval_loss` should not increase (otherwise overfitting)",
   "metadata": {
    "id": "train_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Create SFTTrainer and start training\n# =============================================================================\n# SFTTrainer (Supervised Fine-Tuning Trainer) from TRL library\n# automatically:\n# - Applies chat template to conversations\n# - Masks loss on prompts (trains only on responses)\n# - Manages gradients and optimization\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['test'],\n    processing_class=tokenizer,  # For apply_chat_template\n)\n\nprint(\"Starting training...\")\nprint(f\"   Train examples: {len(dataset['train'])}\")\nprint(f\"   Eval examples: {len(dataset['test'])}\")\nprint(f\"   Estimated time: ~5 minutes on A100\")\nprint(\"-\" * 50)\n\n# Train! This is the main training process\ntrain_result = trainer.train()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Training complete!\")\nprint(f\"   Final loss: {train_result.training_loss:.4f}\")",
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Save Model\n\n**Files saved:**\n- `model.safetensors` - model weights (~540MB)\n- `config.json` - architecture configuration\n- `tokenizer.json`, `tokenizer_config.json` - tokenizer\n- `special_tokens_map.json` - special tokens\n\n**Format:** SafeTensors (safe, no pickle)",
   "metadata": {
    "id": "save_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Save the fine-tuned model to Google Drive\n# =============================================================================\nfrom google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\nFINAL_MODEL_DIR = f\"{OUTPUT_DIR}-final\"\nDRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n\n# Save model weights and config\ntrainer.save_model(FINAL_MODEL_DIR)\n\n# Save tokenizer (needed for inference)\ntokenizer.save_pretrained(FINAL_MODEL_DIR)\n\nprint(f\"Model saved locally to {FINAL_MODEL_DIR}/\")\n\n# Copy to Google Drive\n!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n\nprint(f\"\\nModel copied to Google Drive: {DRIVE_MODEL_DIR}/\")\nprint(\"You can now use this in the conversion notebook!\")\n!ls -la {DRIVE_MODEL_DIR}/",
   "metadata": {
    "id": "save_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Test Fine-tuned Model\n\n**Important:** We test on NEW prompts that were NOT in the training dataset!\n\nWe check:\n- Does the model choose the correct function?\n- Are the arguments correct?\n- Are there any hallucinations?",
   "metadata": {
    "id": "test_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Test the fine-tuned model on new prompts\n# =============================================================================\n# These prompts were NOT in training data - we're testing generalization\n\ntest_prompts = [\n    \"make the background red\",       # change_background_color\n    \"rename the app to Hello World\", # change_app_title  \n    \"show an alert saying welcome\",  # show_alert\n    \"I want a purple background\",    # Variation for color\n    \"set title to My App\",           # Variation for title\n]\n\nprint(\"Testing fine-tuned model:\")\nprint(\"=\" * 60)\n\nfor prompt in test_prompts:\n    # Create conversation in FunctionGemma format\n    messages = [\n        {\"role\": \"developer\", \"content\": DEFAULT_SYSTEM_MSG},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    \n    # apply_chat_template converts messages to tokens with correct markers\n    input_text = tokenizer.apply_chat_template(\n        messages,\n        tools=TOOLS,                    # Pass available functions\n        tokenize=False,                 # Return text, not tokens\n        add_generation_prompt=True      # Add marker for assistant response start\n    )\n    \n    # Tokenize and send to GPU\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,  # Max tokens in response\n        do_sample=False      # Greedy decoding (deterministic)\n    )\n    \n    # Decode only new tokens (without prompt)\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:], \n        skip_special_tokens=False  # Keep special tokens for debugging\n    )\n    \n    print(f\"\\nUser: {prompt}\")\n    print(f\"Model: {response.strip()}\")\n    print(\"-\" * 60)",
   "metadata": {
    "id": "test_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Download Model\n\n**Download ZIP archive** with all model files.\n\n**Next steps after downloading:**\n1. **Convert to TFLite** - using `ai-edge-torch`\n2. **Bundle as .task** - using MediaPipe Model Bundler\n3. **Integrate into Flutter** - put in assets or download from server",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Zip and download the model\n# =============================================================================\n!zip -r {FINAL_MODEL_DIR}.zip {FINAL_MODEL_DIR}/\n\nfrom google.colab import files\nfiles.download(f\"{FINAL_MODEL_DIR}.zip\")\n\nprint(f\"\\nDownload started: {FINAL_MODEL_DIR}.zip\")\nprint(\"\\nNext steps:\")\nprint(\"1. Convert to TFLite using ai-edge-torch\")\nprint(\"2. Bundle as .task using MediaPipe bundler\")\nprint(\"3. Add to flutter_gemma example app\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Push to HuggingFace Hub"
   ],
   "metadata": {
    "id": "hub_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment to push to HuggingFace Hub\n",
    "# Replace 'your-username' with your HuggingFace username\n",
    "\n",
    "# HUB_MODEL_ID = \"your-username/functiongemma-flutter-demo\"\n",
    "# trainer.push_to_hub(HUB_MODEL_ID)\n",
    "# print(f\"âœ… Model pushed to https://huggingface.co/{HUB_MODEL_ID}\")"
   ],
   "metadata": {
    "id": "push_hub"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}