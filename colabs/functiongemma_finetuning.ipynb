{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# FunctionGemma Fine-tuning for Flutter\n\nFine-tuning notebook for [flutter_gemma](https://github.com/DenisovAV/flutter_gemma) plugin.\n\nTrains FunctionGemma 270M for custom function calling:\n- `change_background_color` - Changes app background color\n- `change_app_title` - Changes app title\n- `show_alert` - Shows alert dialog\n\n**Pipeline:**\n1. **This notebook** - Fine-tune model\n2. [functiongemma_to_tflite.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_to_tflite.ipynb) - Convert to TFLite\n3. [functiongemma_tflite_to_task.ipynb](https://github.com/DenisovAV/flutter_gemma/blob/main/colabs/functiongemma_tflite_to_task.ipynb) - Bundle as .task for Flutter\n\n**Requirements:**\n- A100 GPU runtime (Runtime ‚Üí Change runtime type ‚Üí A100)\n- HuggingFace account with accepted [Gemma license](https://huggingface.co/google/functiongemma-270m-it)\n- HuggingFace token with write access",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Install Dependencies\n\n**What we're installing:**\n- `torch` - PyTorch, the core ML framework\n- `transformers` - HuggingFace library for working with LLMs\n- `trl` - Transformer Reinforcement Learning, for SFT (Supervised Fine-Tuning)\n- `datasets` - for dataset handling\n- `accelerate` - for distributed training and GPU optimization\n- `sentencepiece` - tokenizer for Gemma models\n\n**Versions are pinned** for reproducibility.",
   "metadata": {
    "id": "install_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Install dependencies (use Colab's pre-installed torch)\n# =============================================================================\n# Don't reinstall torch - Colab has optimized version pre-installed\n\n!pip install -q transformers==4.57.3 datasets accelerate evaluate trl==0.26.2 protobuf sentencepiece\n!pip install -q huggingface_hub tensorboard\n\nprint(\"\\nDependencies installed!\")",
   "metadata": {
    "id": "install_deps"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2. HuggingFace Authentication\n\n**Setup Colab Secret:**\n1. Click the üîë key icon in the left panel\n2. Add new secret: `HF_TOKEN`\n3. Paste your HuggingFace token (from https://huggingface.co/settings/tokens)\n4. Toggle \"Notebook access\" ON\n\n**Don't forget:** Accept the Gemma license at https://huggingface.co/google/functiongemma-270m-it",
   "metadata": {
    "id": "auth_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Authenticate with HuggingFace using Colab Secrets\n# =============================================================================\nfrom google.colab import userdata\nfrom huggingface_hub import login\n\n# Read token from Colab Secrets (key icon in left panel)\nHF_TOKEN = userdata.get('HF_TOKEN')\n\nif not HF_TOKEN:\n    raise ValueError(\"HF_TOKEN not found in Colab Secrets. Add it via the üîë key icon.\")\n\nlogin(token=HF_TOKEN)\nprint(\"Logged in to HuggingFace!\")",
   "metadata": {
    "id": "auth"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Upload Training Data\n\n**File:** `training_data.jsonl` (320 examples)\n\n**Format per line:**\n```json\n{\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n```\n\n**How to upload:**\n1. Drag and drop the file into the left panel (Files)\n2. Or run this cell - an Upload button will appear",
   "metadata": {
    "id": "upload_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "if not os.path.exists('training_data.jsonl'):\n",
    "    print(\"Please upload training_data.jsonl:\")\n",
    "    uploaded = files.upload()\n",
    "else:\n",
    "    print(\"‚úÖ training_data.jsonl already exists\")\n",
    "\n",
    "# Verify file\n",
    "!wc -l training_data.jsonl\n",
    "!head -1 training_data.jsonl"
   ],
   "metadata": {
    "id": "upload_data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Define Tools and Prepare Dataset\n\n### 4.1 Define Functions (Tools)\n\nWe define Python functions with type hints and docstrings. The `get_json_schema()` utility from HuggingFace transformers automatically generates JSON Schema for each function.\n\n**Why this approach:**\n- Type hints ‚Üí parameter types in schema\n- Docstrings ‚Üí function descriptions\n- No manual JSON writing needed",
   "metadata": {
    "id": "prepare_header"
   }
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datasets import Dataset\nfrom transformers.utils import get_json_schema\n\n# =============================================================================\n# STEP 4.1: Define Python functions for JSON Schema generation\n# =============================================================================\n# These functions are NOT executed - they're only used for JSON Schema generation.\n# get_json_schema() reads the function name, docstring, and type hints,\n# and creates a JSON Schema in OpenAI function calling format.\n\ndef change_background_color(color: str) -> str:\n    \"\"\"Changes the app background color to specified color.\n\n    Args:\n        color: The color name (red, green, blue, yellow, purple, orange)\n    \"\"\"\n    return f\"Changed to {color}\"\n\ndef change_app_title(title: str) -> str:\n    \"\"\"Changes the application title text in the AppBar.\n\n    Args:\n        title: The new title text to display\n    \"\"\"\n    return f\"Title set to {title}\"\n\ndef show_alert(title: str, message: str) -> str:\n    \"\"\"Shows an alert dialog with a custom message and title.\n\n    Args:\n        title: The title of the alert dialog\n        message: The message content of the alert dialog\n    \"\"\"\n    return f\"Alert shown: {title}\"\n\n# =============================================================================\n# Generate JSON Schemas from Python functions\n# =============================================================================\n# get_json_schema() creates a structure like:\n# {\"type\": \"function\", \"function\": {\"name\": \"...\", \"description\": \"...\", \"parameters\": {...}}}\n\nTOOLS = [\n    get_json_schema(change_background_color),\n    get_json_schema(change_app_title),\n    get_json_schema(show_alert),\n]\n\nprint(\"Tools defined:\")\nfor tool in TOOLS:\n    print(f\"   - {tool['function']['name']}: {tool['function']['description'][:50]}...\")",
   "metadata": {
    "id": "define_tools"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 Convert Dataset to Google FunctionGemma Format\n\nWe use the **official Google FunctionGemma format** as documented at:\nhttps://huggingface.co/google/functiongemma-270m-it\n\nWe do NOT use HuggingFace's `apply_chat_template` because it produces a different format\nthat breaks compatibility with the base model.\n\n**Google FunctionGemma format:**\n\n**Input (prompt):**\n```\n<start_of_turn>developer\nYou are a model that can do function calling with the following functions\n<start_function_declaration>declaration:function_name{description:<escape>...<escape>,parameters:{...}}<end_function_declaration>\n<end_of_turn>\n<start_of_turn>user\nmake it red\n<end_of_turn>\n<start_of_turn>model\n```\n\n**Output (completion):**\n```\n<start_function_call>call:function_name{param:<escape>value<escape>}<end_function_call>\n```\n\n**Key format elements:**\n- `<escape>` tokens wrap all string values (not JSON quotes)\n- `call:` prefix before function name in output\n- Parameters use `{param:<escape>value<escape>}` format",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# STEP 4.2: Convert dataset to Google FunctionGemma format\n# =============================================================================\n# CRITICAL: We manually create prompts in the EXACT format Flutter uses!\n# Do NOT use apply_chat_template - it produces a different format.\n\nimport json\nfrom datasets import Dataset\n\n# FunctionGemma special tokens (same as Flutter uses)\nSTART_TURN = \"<start_of_turn>\"\nEND_TURN = \"<end_of_turn>\"\nSTART_DECL = \"<start_function_declaration>\"\nEND_DECL = \"<end_function_declaration>\"\nSTART_CALL = \"<start_function_call>\"\nEND_CALL = \"<end_function_call>\"\nESCAPE = \"<escape>\"\n\n# =============================================================================\n# Function declarations in Google format (same as Flutter's _createFunctionGemmaToolsPrompt)\n# =============================================================================\nFUNCTION_DECLARATIONS = f\"\"\"{START_DECL}declaration:change_background_color{{description:{ESCAPE}Changes the app background color{ESCAPE},parameters:{{properties:{{color:{{description:{ESCAPE}The color name (red, green, blue, yellow, purple, orange){ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}color{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\n{START_DECL}declaration:change_app_title{{description:{ESCAPE}Changes the application title text in the AppBar{ESCAPE},parameters:{{properties:{{title:{{description:{ESCAPE}The new title text to display{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}title{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\n{START_DECL}declaration:show_alert{{description:{ESCAPE}Shows an alert dialog with a custom message and title{ESCAPE},parameters:{{properties:{{title:{{description:{ESCAPE}The title of the alert dialog{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}},message:{{description:{ESCAPE}The message content of the alert dialog{ESCAPE},type:{ESCAPE}STRING{ESCAPE}}}}},required:[{ESCAPE}title{ESCAPE},{ESCAPE}message{ESCAPE}],type:{ESCAPE}OBJECT{ESCAPE}}}}}{END_DECL}\"\"\"\n\nSYSTEM_PROMPT = f\"\"\"{START_TURN}developer\nYou are a model that can do function calling with the following functions\n{FUNCTION_DECLARATIONS}\n{END_TURN}\n\"\"\"\n\ndef create_training_example(sample):\n    \"\"\"\n    Creates training example in exact Google FunctionGemma format.\n    \n    Input: {\"user_content\": \"make it red\", \"tool_name\": \"change_background_color\", \"tool_arguments\": \"{\\\"color\\\": \\\"red\\\"}\"}\n    \n    Output text for training:\n    <start_of_turn>developer\n    You are a model...\n    <end_of_turn>\n    <start_of_turn>user\n    make it red\n    <end_of_turn>\n    <start_of_turn>model\n    <start_function_call>call:change_background_color{color:<escape>red<escape>}<end_function_call>\n    \"\"\"\n    user_content = sample[\"user_content\"]\n    tool_name = sample[\"tool_name\"]\n    tool_args = json.loads(sample[\"tool_arguments\"])\n    \n    # Build prompt (input)\n    prompt = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n{user_content}\n{END_TURN}\n{START_TURN}model\n\"\"\"\n    \n    # Build completion (output) in Google format: {param:<escape>value<escape>}\n    params_str = \",\".join([f\"{k}:{ESCAPE}{v}{ESCAPE}\" for k, v in tool_args.items()])\n    completion = f\"{START_CALL}call:{tool_name}{{{params_str}}}{END_CALL}\"\n    \n    # Full training text = prompt + completion\n    return {\"text\": prompt + completion}\n\n# =============================================================================\n# Load and convert dataset\n# =============================================================================\nraw_data = []\nwith open('training_data.jsonl', 'r') as f:\n    for line in f:\n        raw_data.append(json.loads(line.strip()))\n\nprint(f\"Loaded {len(raw_data)} raw examples\")\n\ndataset = Dataset.from_list(raw_data)\ndataset = dataset.map(create_training_example, remove_columns=dataset.features)\n\n# Split into train/test (90%/10%)\ndataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n\nprint(f\"\\nDataset prepared:\")\nprint(f\"   Train: {len(dataset['train'])} examples\")\nprint(f\"   Test:  {len(dataset['test'])} examples\")\n\n# Show sample\nprint(f\"\\n{'='*60}\")\nprint(\"Sample training example:\")\nprint(\"=\"*60)\nprint(dataset['train'][0]['text'][:800])\nprint(\"...\")",
   "metadata": {
    "id": "load_dataset"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load Base Model\n\n**Model:** `google/functiongemma-270m-it`\n- 270M parameters (compact, designed for on-device)\n- Instruction-tuned (it) - already trained to follow instructions\n- Specialized for function calling\n\n**Loading parameters:**\n- `torch_dtype=bfloat16` - 16-bit weights to save memory (~540MB instead of ~1GB)\n- `device_map=\"auto\"` - automatically load to GPU\n- `attn_implementation=\"eager\"` - without FlashAttention (for compatibility)",
   "metadata": {
    "id": "model_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# =============================================================================\n# Load FunctionGemma base model\n# =============================================================================\nBASE_MODEL = \"google/functiongemma-270m-it\"\n\nprint(f\"Loading {BASE_MODEL}...\")\nprint(\"   (Downloads ~540MB on first run, then uses cache)\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.bfloat16,      # 16-bit to save VRAM\n    device_map=\"auto\",                # Automatically load to GPU\n    attn_implementation=\"eager\"       # Without FlashAttention for compatibility\n)\n\n# Tokenizer converts text to tokens and back\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nprint(f\"\\nModel loaded!\")\nprint(f\"   Parameters: {model.num_parameters():,}\")\nprint(f\"   Memory: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")\nprint(f\"   Device: {model.device}\")",
   "metadata": {
    "id": "load_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Configure Training\n\n**Hyperparameters from official Google FunctionGemma cookbook:**\n\n| Parameter | Value | Explanation |\n|-----------|-------|-------------|\n| `num_train_epochs` | 5 | Extended training for enum support (320 examples) |\n| `learning_rate` | 1e-5 | Learning rate (conservative for fine-tuning) |\n| `lr_scheduler_type` | cosine | Smoothly decreases LR towards end of training |\n| `gradient_accumulation_steps` | 8 | Gradient accumulation (effective batch = 32) |\n| `max_length` | 1024 | Maximum sequence length in tokens |\n| `bf16` | True | 16-bit training to save memory |\n\n**Why these values:**\n- LR 1e-5 (not 5e-5) - prevents \"forgetting\" base knowledge\n- Cosine scheduler - smooth LR decay improves convergence\n- Gradient accumulation 8 - simulates large batch without running out of memory",
   "metadata": {
    "id": "config_header"
   }
  },
  {
   "cell_type": "code",
   "source": "from trl import SFTConfig, SFTTrainer\n\n# Output directory\nOUTPUT_DIR = \"functiongemma-flutter-demo\"\n\n# =============================================================================\n# Training configuration (based on official Google FunctionGemma cookbook)\n# https://github.com/google-gemini/gemma-cookbook/blob/main/FunctionGemma/\n# =============================================================================\ntraining_args = SFTConfig(\n    output_dir=OUTPUT_DIR,\n\n    # Dataset field with pre-formatted Google FunctionGemma format\n    dataset_text_field=\"text\",          # Use our pre-formatted text, NOT apply_chat_template\n\n    # Training params (Google official uses 2 epochs, we use 5 for enum support)\n    max_length=1024,                    # Max sequence length in tokens\n    packing=False,                      # Don't pack multiple examples into one sequence\n    num_train_epochs=5,                 # Extended training for enum support (320 examples)\n    per_device_train_batch_size=4,      # Batch size per GPU\n    per_device_eval_batch_size=4,       # Eval batch size\n    gradient_accumulation_steps=8,      # Effective batch size: 4 * 8 = 32\n\n    # Optimizer (Google official params)\n    learning_rate=1e-5,                 # Google official: 1e-5 (more conservative than 5e-5)\n    lr_scheduler_type=\"cosine\",         # Google official: cosine decay\n    optim=\"adamw_torch_fused\",          # Fused AdamW for faster training\n    warmup_ratio=0.1,                   # 10% warmup steps\n\n    # Logging and checkpoints\n    logging_steps=10,                   # Log every 10 steps\n    eval_strategy=\"epoch\",              # Evaluate after each epoch\n    save_strategy=\"epoch\",              # Save checkpoint after each epoch\n\n    # Memory optimization\n    gradient_checkpointing=False,       # Trade compute for memory (enable if OOM)\n    bf16=True,                          # Use bfloat16 for training\n\n    # Output\n    report_to=\"tensorboard\",            # Log to TensorBoard\n    push_to_hub=False,                  # Set to True to upload to HuggingFace\n)\n\nprint(\"Training configuration (Google official params):\")\nprint(f\"   Epochs: {training_args.num_train_epochs}\")\nprint(f\"   Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\nprint(f\"   LR scheduler: {training_args.lr_scheduler_type}\")\nprint(f\"   Max length: {training_args.max_length}\")\nprint(f\"   Dataset field: {training_args.dataset_text_field}\")",
   "metadata": {
    "id": "config_training"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Start Training\n\n**What happens:**\n1. `SFTTrainer` uses the pre-formatted `text` field from our dataset\n2. Model learns to predict the correct function call for each user message\n3. We do NOT use `apply_chat_template` - data is already in correct Google format!\n\n**Training time:** ~5 minutes on A100 GPU (for ~300 examples)\n\n**Monitoring:**\n- `loss` should decrease\n- `eval_loss` should not increase (otherwise overfitting)",
   "metadata": {
    "id": "train_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Create SFTTrainer and start training\n# =============================================================================\n# dataset_text_field=\"text\" is configured in SFTConfig above.\n# This tells SFTTrainer to use our pre-formatted text directly,\n# without applying HuggingFace's chat template (which uses wrong format).\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['test'],\n    processing_class=tokenizer,  # TRL 0.26.2: use processing_class, not tokenizer\n)\n\nprint(\"Starting training...\")\nprint(f\"   Train examples: {len(dataset['train'])}\")\nprint(f\"   Eval examples: {len(dataset['test'])}\")\nprint(f\"   Format: Google FunctionGemma (manual)\")\nprint(f\"   Estimated time: ~5 minutes on A100\")\nprint(\"-\" * 50)\n\n# Train!\ntrain_result = trainer.train()\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Training complete!\")\nprint(f\"   Final loss: {train_result.training_loss:.4f}\")",
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Save Model\n\n**Files saved:**\n- `model.safetensors` - model weights (~540MB)\n- `config.json` - architecture configuration\n- `tokenizer.json`, `tokenizer_config.json` - tokenizer\n- `special_tokens_map.json` - special tokens\n\n**Format:** SafeTensors (safe, no pickle)",
   "metadata": {
    "id": "save_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Save the fine-tuned model to Google Drive\n# =============================================================================\nfrom google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\nFINAL_MODEL_DIR = f\"{OUTPUT_DIR}-final\"\nDRIVE_MODEL_DIR = f\"/content/drive/MyDrive/{FINAL_MODEL_DIR}\"\n\n# Save model weights and config\ntrainer.save_model(FINAL_MODEL_DIR)\n\n# Save tokenizer (needed for inference)\ntokenizer.save_pretrained(FINAL_MODEL_DIR)\n\nprint(f\"Model saved locally to {FINAL_MODEL_DIR}/\")\n\n# Copy to Google Drive\n!cp -r {FINAL_MODEL_DIR} /content/drive/MyDrive/\n\nprint(f\"\\nModel copied to Google Drive: {DRIVE_MODEL_DIR}/\")\nprint(\"You can now use this in the conversion notebook!\")\n!ls -la {DRIVE_MODEL_DIR}/",
   "metadata": {
    "id": "save_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Test Fine-tuned Model\n\n**Important:** We test on NEW prompts that were NOT in the training dataset!\n\nWe check:\n- Does the model choose the correct function?\n- Are the arguments correct?\n- Are there any hallucinations?",
   "metadata": {
    "id": "test_header"
   }
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Test the fine-tuned model on new prompts\n# =============================================================================\n# CRITICAL: Use the same Google format as training (not apply_chat_template!)\n\ntest_prompts = [\n    \"make the background red\",       # change_background_color\n    \"rename the app to Hello World\", # change_app_title  \n    \"show an alert saying welcome\",  # show_alert\n    \"I want a purple background\",    # Variation for color\n    \"set title to My App\",           # Variation for title\n]\n\nprint(\"Testing fine-tuned model:\")\nprint(\"=\" * 60)\n\nfor prompt in test_prompts:\n    # Create prompt in SAME format as training (Google FunctionGemma)\n    input_text = f\"\"\"{SYSTEM_PROMPT}{START_TURN}user\n{prompt}\n{END_TURN}\n{START_TURN}model\n\"\"\"\n    \n    # Tokenize and send to GPU\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    \n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        do_sample=False,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    \n    # Decode only new tokens (without prompt)\n    response = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:], \n        skip_special_tokens=False\n    )\n    \n    print(f\"\\nUser: {prompt}\")\n    print(f\"Model: {response.strip()}\")\n    \n    # Verify format\n    if \"<start_function_call>call:\" in response:\n        print(\"   ‚úÖ Correct format!\")\n    else:\n        print(\"   ‚ö†Ô∏è  Unexpected format\")\n    print(\"-\" * 60)",
   "metadata": {
    "id": "test_model"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Download Model\n\n**Download ZIP archive** with all model files.\n\n**Next steps after downloading:**\n1. **Convert to TFLite** - using `ai-edge-torch`\n2. **Bundle as .task** - using MediaPipe Model Bundler\n3. **Integrate into Flutter** - put in assets or download from server",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Zip and download the model\n# =============================================================================\n!zip -r {FINAL_MODEL_DIR}.zip {FINAL_MODEL_DIR}/\n\nfrom google.colab import files\nfiles.download(f\"{FINAL_MODEL_DIR}.zip\")\n\nprint(f\"\\nDownload started: {FINAL_MODEL_DIR}.zip\")\nprint(\"\\nNext steps:\")\nprint(\"1. Convert to TFLite using ai-edge-torch\")\nprint(\"2. Bundle as .task using MediaPipe bundler\")\nprint(\"3. Add to flutter_gemma example app\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Push to HuggingFace Hub"
   ],
   "metadata": {
    "id": "hub_header"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Uncomment to push to HuggingFace Hub\n",
    "# Replace 'your-username' with your HuggingFace username\n",
    "\n",
    "# HUB_MODEL_ID = \"your-username/functiongemma-flutter-demo\"\n",
    "# trainer.push_to_hub(HUB_MODEL_ID)\n",
    "# print(f\"‚úÖ Model pushed to https://huggingface.co/{HUB_MODEL_ID}\")"
   ],
   "metadata": {
    "id": "push_hub"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}